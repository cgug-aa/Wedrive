{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert 모델 성능 평가\n",
    "사용 데이터: od_uuid/2023/000c16dad0a74e3aa088fc8616b4b220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import folium\n",
    "import geopy.distance\n",
    "\n",
    "os.chdir('../')\n",
    "data_path=os.getcwd()+'/data/od_uuid/2023'\n",
    "\n",
    "#데이터셋 칼럼명 추가\n",
    "column_name=['id','start_time','end_time','start_lat','start_lng','end_lat','end_lng','?1','?2','?3']\n",
    "\n",
    "raw_data=pd.read_csv(data_path+'/000c16dad0a74e3aa088fc8616b4b220.csv')\n",
    "data=raw_data.values.tolist()\n",
    "dataframe=pd.DataFrame(data, columns=column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test 데이터 분할\n",
    "datafilter1=dataframe['start_time'].str.contains(\"2023-10\")\n",
    "datafilter2=dataframe['start_time'].str.contains(\"2023-11\")\n",
    "datafilter3=dataframe['start_time'].str.contains(\"2023-12\")\n",
    "\n",
    "train_data=dataframe[~(datafilter1|datafilter2|datafilter3)]\n",
    "test_data=dataframe[datafilter1|datafilter2|datafilter3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리드 생성을 위한 함수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to convert numbers into corresponding letter labels\n",
    "def num_to_letter(num):\n",
    "    '''\n",
    "    num         : number that we have to convert\n",
    "    '''\n",
    "    return string.ascii_uppercase[num]\n",
    "\n",
    "def generate_initial_grids():\n",
    "        \"\"\"초기 그리드 생성\"\"\"\n",
    "        south, west, north, east = south_korea_bounds\n",
    "        lat_step = (north - south) / grid_size\n",
    "        lon_step = (east - west) / grid_size\n",
    "        grid_queue = []\n",
    "\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                grid_south = south + i * lat_step\n",
    "                grid_north = south + (i + 1) * lat_step\n",
    "                grid_west = west + j * lon_step\n",
    "                grid_east = west + (j + 1) * lon_step\n",
    "                grid_queue.append((grid_south, grid_west, grid_north, grid_east, \n",
    "                                   num_to_letter(i) + num_to_letter(j)))\n",
    "\n",
    "        return grid_queue\n",
    "\n",
    "# Create a function to get the grid label of the coordinate point\n",
    "def get_grid_label(lat, lng, final_grids):\n",
    "    '''\n",
    "    lat         : latitude\n",
    "    lng         : longitude\n",
    "    final_grids : all cells and their minimum/maximum latitude/longitude\n",
    "    '''\n",
    "    for south, west, north, east, grid_label in final_grids:\n",
    "        if south <= lat <= north and west <= lng <= east:\n",
    "            return grid_label\n",
    "    return None\n",
    "\n",
    "def is_path_in_grid(south, west, north, east, path_points):\n",
    "        \"\"\"경로가 그리드 안에 있는지 확인\"\"\"\n",
    "        return any(south <= lat <= north and west <= lng <= east for lat, lng in path_points)\n",
    "\n",
    "def subdivide_grids(grid_queue, path_points):\n",
    "    \"\"\"그리드 분할\"\"\"\n",
    "    final_grids = []\n",
    "\n",
    "    while grid_queue:\n",
    "        south, west, north, east, grid_label = grid_queue.pop(0)\n",
    "        grid_size_km = min(geopy.distance.distance((south, west), (south, east)).km,\n",
    "                           geopy.distance.distance((south, west), (north, west)).km)\n",
    "            \n",
    "        if grid_size_km > min_size_km and is_path_in_grid(south, west, north, east, path_points):\n",
    "            mid_lat = (south + north) / 2\n",
    "            mid_lon = (west + east) / 2\n",
    "            grid_queue.append((south, west, mid_lat, mid_lon, grid_label + 'C'))\n",
    "            grid_queue.append((mid_lat, west, north, mid_lon, grid_label + 'A'))\n",
    "            grid_queue.append((south, mid_lon, mid_lat, east, grid_label + 'D'))\n",
    "            grid_queue.append((mid_lat, mid_lon, north, east, grid_label + 'B'))\n",
    "        else:\n",
    "            final_grids.append((south, west, north, east, grid_label))\n",
    "\n",
    "    return final_grids\n",
    "\n",
    "# Approximate border coordinates of South Korea\n",
    "south_korea_bounds = [33.10, 124.57, 38.60, 131]\n",
    "min_size_km = 0.76\n",
    "grid_size = 13\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train 데이터 시작/종료 위치 그리드 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11\\AppData\\Local\\Temp\\ipykernel_16728\\1362932771.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['start_grid']=train_data.apply(lambda row: get_grid_label(row['start_lat'], row['start_lng'], start_final_grids), axis=1)\n",
      "C:\\Users\\11\\AppData\\Local\\Temp\\ipykernel_16728\\1362932771.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['end_grid']=train_data.apply(lambda row: get_grid_label(row['end_lat'], row['end_lng'], end_final_grids), axis=1)\n"
     ]
    }
   ],
   "source": [
    "start_points=train_data[['start_lat','start_lng']].values.tolist()\n",
    "start_grid_queue = generate_initial_grids()\n",
    "start_final_grids = subdivide_grids(start_grid_queue, start_points)\n",
    "\n",
    "end_points=train_data[['end_lat','end_lng']].values.tolist()\n",
    "end_grid_queue = generate_initial_grids()\n",
    "end_final_grids = subdivide_grids(end_grid_queue, end_points)\n",
    "\n",
    "train_data['start_grid']=train_data.apply(lambda row: get_grid_label(row['start_lat'], row['start_lng'], start_final_grids), axis=1)\n",
    "train_data['end_grid']=train_data.apply(lambda row: get_grid_label(row['end_lat'], row['end_lng'], end_final_grids), axis=1)\n",
    "train_data=train_data[['start_grid', 'end_grid']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 데이터 시작/종료 위치 그리드 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11\\AppData\\Local\\Temp\\ipykernel_16728\\3916890138.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['start_grid']=test_data.apply(lambda row: get_grid_label(row['start_lat'], row['start_lng'], start_final_grids), axis=1)\n",
      "C:\\Users\\11\\AppData\\Local\\Temp\\ipykernel_16728\\3916890138.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['end_grid']=test_data.apply(lambda row: get_grid_label(row['end_lat'], row['end_lng'], end_final_grids), axis=1)\n"
     ]
    }
   ],
   "source": [
    "start_points=test_data[['start_lat','start_lng']].values.tolist()\n",
    "start_grid_queue = generate_initial_grids()\n",
    "start_final_grids = subdivide_grids(start_grid_queue, start_points)\n",
    "\n",
    "end_points=test_data[['end_lat','end_lng']].values.tolist()\n",
    "end_grid_queue = generate_initial_grids()\n",
    "end_final_grids = subdivide_grids(end_grid_queue, end_points)\n",
    "\n",
    "test_data['start_grid']=test_data.apply(lambda row: get_grid_label(row['start_lat'], row['start_lng'], start_final_grids), axis=1)\n",
    "test_data['end_grid']=test_data.apply(lambda row: get_grid_label(row['end_lat'], row['end_lng'], end_final_grids), axis=1)\n",
    "test_data=test_data[['start_grid', 'end_grid']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_grid</th>\n",
       "      <th>end_grid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KGCACADB</td>\n",
       "      <td>KGCACBAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KGCACBAD</td>\n",
       "      <td>KGCACBBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KGCABCBD</td>\n",
       "      <td>KGCACBBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KGCACBAD</td>\n",
       "      <td>KGACCDDC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KGCABACC</td>\n",
       "      <td>KGCACBBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>KFBBCDAD</td>\n",
       "      <td>KFBBCDAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>KFBBCDAB</td>\n",
       "      <td>KFBBCDAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>KFBBCDAD</td>\n",
       "      <td>KFBBCDAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>KFBBCDAD</td>\n",
       "      <td>KFBBCDAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>KFBBCDAD</td>\n",
       "      <td>KFBBCDAB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>882 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    start_grid  end_grid\n",
       "0     KGCACADB  KGCACBAD\n",
       "1     KGCACBAD  KGCACBBB\n",
       "2     KGCABCBD  KGCACBBC\n",
       "3     KGCACBAD  KGACCDDC\n",
       "4     KGCABACC  KGCACBBB\n",
       "..         ...       ...\n",
       "877   KFBBCDAD  KFBBCDAD\n",
       "878   KFBBCDAB  KFBBCDAC\n",
       "879   KFBBCDAD  KFBBCDAD\n",
       "880   KFBBCDAD  KFBBCDAD\n",
       "881   KFBBCDAD  KFBBCDAB\n",
       "\n",
       "[882 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert 모델 테스팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\11\\anaconda3\\envs\\wedrive\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 4.224711894989014\n",
      "Epoch: 0, Loss: 4.0392045974731445\n",
      "Epoch: 0, Loss: 3.7075023651123047\n",
      "Epoch: 0, Loss: 4.052983283996582\n",
      "Epoch: 0, Loss: 3.53875732421875\n",
      "Epoch: 0, Loss: 3.332005023956299\n",
      "Epoch: 0, Loss: 3.4924941062927246\n",
      "Epoch: 0, Loss: 2.554784059524536\n",
      "Epoch: 0, Loss: 3.19659161567688\n",
      "Epoch: 0, Loss: 2.8790833950042725\n",
      "Epoch: 0, Loss: 3.2511305809020996\n",
      "Epoch: 0, Loss: 3.752807855606079\n",
      "Epoch: 0, Loss: 3.4340105056762695\n",
      "Epoch: 0, Loss: 2.4506208896636963\n",
      "Epoch: 0, Loss: 3.4043328762054443\n",
      "Epoch: 0, Loss: 2.3500301837921143\n",
      "Epoch: 0, Loss: 4.26141357421875\n",
      "Epoch: 0, Loss: 0.9881395697593689\n",
      "Epoch: 0, Loss: 3.017796754837036\n",
      "Epoch: 0, Loss: 1.9529849290847778\n",
      "Epoch: 0, Loss: 1.924493670463562\n",
      "Epoch: 0, Loss: 1.442360281944275\n",
      "Epoch: 0, Loss: 3.299684762954712\n",
      "Epoch: 0, Loss: 2.1568853855133057\n",
      "Epoch: 0, Loss: 1.3945114612579346\n",
      "Epoch: 0, Loss: 3.3202497959136963\n",
      "Epoch: 0, Loss: 2.2900595664978027\n",
      "Epoch: 0, Loss: 2.692734718322754\n",
      "Epoch: 0, Loss: 1.6136741638183594\n",
      "Epoch: 0, Loss: 1.715301275253296\n",
      "Epoch: 0, Loss: 1.15902841091156\n",
      "Epoch: 0, Loss: 1.6781237125396729\n",
      "Epoch: 0, Loss: 2.64507794380188\n",
      "Epoch: 0, Loss: 1.8778983354568481\n",
      "Epoch: 0, Loss: 2.750825881958008\n",
      "Epoch: 0, Loss: 1.6164122819900513\n",
      "Epoch: 0, Loss: 2.081348419189453\n",
      "Epoch: 0, Loss: 1.52427339553833\n",
      "Epoch: 0, Loss: 1.293657898902893\n",
      "Epoch: 0, Loss: 2.7753591537475586\n",
      "Epoch: 0, Loss: 3.2530345916748047\n",
      "Epoch: 0, Loss: 2.5936734676361084\n",
      "Epoch: 0, Loss: 1.891614556312561\n",
      "Epoch: 0, Loss: 1.0925713777542114\n",
      "Epoch: 0, Loss: 3.389118194580078\n",
      "Epoch: 0, Loss: 2.157970666885376\n",
      "Epoch: 0, Loss: 1.4544625282287598\n",
      "Epoch: 0, Loss: 1.3544613122940063\n",
      "Epoch: 0, Loss: 2.4783153533935547\n",
      "Epoch: 0, Loss: 1.911487102508545\n",
      "Epoch: 0, Loss: 0.6667018532752991\n",
      "Epoch: 0, Loss: 1.7368923425674438\n",
      "Epoch: 0, Loss: 0.8059271574020386\n",
      "Epoch: 0, Loss: 2.4839577674865723\n",
      "Epoch: 0, Loss: 2.727881669998169\n",
      "Epoch: 0, Loss: 2.2853405475616455\n",
      "Epoch: 0, Loss: 2.075526714324951\n",
      "Epoch: 0, Loss: 1.3774254322052002\n",
      "Epoch: 0, Loss: 2.380755662918091\n",
      "Epoch: 0, Loss: 2.3869903087615967\n",
      "Epoch: 0, Loss: 2.016634941101074\n",
      "Epoch: 0, Loss: 2.0811607837677\n",
      "Epoch: 0, Loss: 1.940767765045166\n",
      "Epoch: 0, Loss: 2.291985511779785\n",
      "Epoch: 0, Loss: 1.9648473262786865\n",
      "Epoch: 0, Loss: 2.1832876205444336\n",
      "Epoch: 0, Loss: 2.4407906532287598\n",
      "Epoch: 0, Loss: 0.8265982270240784\n",
      "Epoch: 0, Loss: 1.4631909132003784\n",
      "Epoch: 0, Loss: 1.540221095085144\n",
      "Epoch: 0, Loss: 1.9993815422058105\n",
      "Epoch: 0, Loss: 1.4813690185546875\n",
      "Epoch: 0, Loss: 1.302810549736023\n",
      "Epoch: 0, Loss: 1.955895185470581\n",
      "Epoch: 0, Loss: 1.904926061630249\n",
      "Epoch: 0, Loss: 2.7936911582946777\n",
      "Epoch: 0, Loss: 0.8227996826171875\n",
      "Epoch: 0, Loss: 3.3580868244171143\n",
      "Epoch: 0, Loss: 1.8900511264801025\n",
      "Epoch: 0, Loss: 2.682530403137207\n",
      "Epoch: 0, Loss: 3.1533782482147217\n",
      "Epoch: 0, Loss: 1.5518136024475098\n",
      "Epoch: 0, Loss: 2.3736720085144043\n",
      "Epoch: 0, Loss: 1.6451752185821533\n",
      "Epoch: 0, Loss: 0.9766048789024353\n",
      "Epoch: 0, Loss: 2.8296291828155518\n",
      "Epoch: 0, Loss: 3.0106258392333984\n",
      "Epoch: 0, Loss: 2.4354798793792725\n",
      "Epoch: 0, Loss: 2.2946739196777344\n",
      "Epoch: 0, Loss: 1.8394663333892822\n",
      "Epoch: 0, Loss: 3.6536316871643066\n",
      "Epoch: 0, Loss: 2.3595352172851562\n",
      "Epoch: 0, Loss: 1.3867779970169067\n",
      "Epoch: 0, Loss: 1.921818494796753\n",
      "Epoch: 0, Loss: 1.6022969484329224\n",
      "Epoch: 0, Loss: 1.602120280265808\n",
      "Epoch: 0, Loss: 1.2502073049545288\n",
      "Epoch: 0, Loss: 2.5540192127227783\n",
      "Epoch: 0, Loss: 1.2537221908569336\n",
      "Epoch: 0, Loss: 1.589898943901062\n",
      "Epoch: 0, Loss: 2.18951153755188\n",
      "Epoch: 0, Loss: 1.7877002954483032\n",
      "Epoch: 0, Loss: 1.6340000629425049\n",
      "Epoch: 0, Loss: 2.2663562297821045\n",
      "Epoch: 0, Loss: 2.8431777954101562\n",
      "Epoch: 0, Loss: 1.9412251710891724\n",
      "Epoch: 0, Loss: 3.2096617221832275\n",
      "Epoch: 0, Loss: 1.5312256813049316\n",
      "Epoch: 0, Loss: 1.967850685119629\n",
      "Epoch: 0, Loss: 2.3573553562164307\n",
      "Epoch: 0, Loss: 0.3084369897842407\n",
      "Epoch: 1, Loss: 2.5121798515319824\n",
      "Epoch: 1, Loss: 2.5287837982177734\n",
      "Epoch: 1, Loss: 0.9438096284866333\n",
      "Epoch: 1, Loss: 2.3573837280273438\n",
      "Epoch: 1, Loss: 0.7487602829933167\n",
      "Epoch: 1, Loss: 2.3531858921051025\n",
      "Epoch: 1, Loss: 1.0118155479431152\n",
      "Epoch: 1, Loss: 1.5647475719451904\n",
      "Epoch: 1, Loss: 3.026885986328125\n",
      "Epoch: 1, Loss: 1.0697646141052246\n",
      "Epoch: 1, Loss: 1.798382043838501\n",
      "Epoch: 1, Loss: 2.612470865249634\n",
      "Epoch: 1, Loss: 1.689270257949829\n",
      "Epoch: 1, Loss: 1.6517677307128906\n",
      "Epoch: 1, Loss: 1.7271761894226074\n",
      "Epoch: 1, Loss: 1.8907158374786377\n",
      "Epoch: 1, Loss: 2.1530966758728027\n",
      "Epoch: 1, Loss: 0.7056219577789307\n",
      "Epoch: 1, Loss: 2.0568630695343018\n",
      "Epoch: 1, Loss: 1.6360182762145996\n",
      "Epoch: 1, Loss: 1.4493168592453003\n",
      "Epoch: 1, Loss: 2.7683966159820557\n",
      "Epoch: 1, Loss: 1.5616350173950195\n",
      "Epoch: 1, Loss: 2.9097909927368164\n",
      "Epoch: 1, Loss: 2.708540916442871\n",
      "Epoch: 1, Loss: 1.835598349571228\n",
      "Epoch: 1, Loss: 1.0835635662078857\n",
      "Epoch: 1, Loss: 2.0458824634552\n",
      "Epoch: 1, Loss: 1.1966530084609985\n",
      "Epoch: 1, Loss: 2.355957508087158\n",
      "Epoch: 1, Loss: 1.8758862018585205\n",
      "Epoch: 1, Loss: 1.0268045663833618\n",
      "Epoch: 1, Loss: 1.669277310371399\n",
      "Epoch: 1, Loss: 1.7266364097595215\n",
      "Epoch: 1, Loss: 1.5303325653076172\n",
      "Epoch: 1, Loss: 2.275473117828369\n",
      "Epoch: 1, Loss: 2.0684354305267334\n",
      "Epoch: 1, Loss: 2.677370071411133\n",
      "Epoch: 1, Loss: 2.468899726867676\n",
      "Epoch: 1, Loss: 2.3998632431030273\n",
      "Epoch: 1, Loss: 0.9528838396072388\n",
      "Epoch: 1, Loss: 1.8190770149230957\n",
      "Epoch: 1, Loss: 1.6132982969284058\n",
      "Epoch: 1, Loss: 1.918904423713684\n",
      "Epoch: 1, Loss: 2.4870855808258057\n",
      "Epoch: 1, Loss: 2.168097496032715\n",
      "Epoch: 1, Loss: 1.3316667079925537\n",
      "Epoch: 1, Loss: 2.214107036590576\n",
      "Epoch: 1, Loss: 0.8779993653297424\n",
      "Epoch: 1, Loss: 2.238666296005249\n",
      "Epoch: 1, Loss: 2.2387337684631348\n",
      "Epoch: 1, Loss: 2.551426887512207\n",
      "Epoch: 1, Loss: 2.481273651123047\n",
      "Epoch: 1, Loss: 2.8223514556884766\n",
      "Epoch: 1, Loss: 2.3021490573883057\n",
      "Epoch: 1, Loss: 1.82207190990448\n",
      "Epoch: 1, Loss: 1.7321503162384033\n",
      "Epoch: 1, Loss: 1.5858992338180542\n",
      "Epoch: 1, Loss: 1.8621869087219238\n",
      "Epoch: 1, Loss: 1.5301814079284668\n",
      "Epoch: 1, Loss: 0.6093785762786865\n",
      "Epoch: 1, Loss: 1.6884955167770386\n",
      "Epoch: 1, Loss: 1.9879993200302124\n",
      "Epoch: 1, Loss: 1.966920256614685\n",
      "Epoch: 1, Loss: 1.2544403076171875\n",
      "Epoch: 1, Loss: 2.776728868484497\n",
      "Epoch: 1, Loss: 2.2907485961914062\n",
      "Epoch: 1, Loss: 1.8554058074951172\n",
      "Epoch: 1, Loss: 1.387054681777954\n",
      "Epoch: 1, Loss: 2.021096706390381\n",
      "Epoch: 1, Loss: 1.9270187616348267\n",
      "Epoch: 1, Loss: 2.212059497833252\n",
      "Epoch: 1, Loss: 1.5866566896438599\n",
      "Epoch: 1, Loss: 3.115356922149658\n",
      "Epoch: 1, Loss: 2.3998942375183105\n",
      "Epoch: 1, Loss: 1.5485422611236572\n",
      "Epoch: 1, Loss: 2.1881332397460938\n",
      "Epoch: 1, Loss: 1.6756722927093506\n",
      "Epoch: 1, Loss: 3.369800329208374\n",
      "Epoch: 1, Loss: 1.7539197206497192\n",
      "Epoch: 1, Loss: 1.8816137313842773\n",
      "Epoch: 1, Loss: 1.4486802816390991\n",
      "Epoch: 1, Loss: 2.0118558406829834\n",
      "Epoch: 1, Loss: 2.556079149246216\n",
      "Epoch: 1, Loss: 1.427720069885254\n",
      "Epoch: 1, Loss: 1.9364835023880005\n",
      "Epoch: 1, Loss: 1.9591797590255737\n",
      "Epoch: 1, Loss: 2.4336307048797607\n",
      "Epoch: 1, Loss: 2.638375759124756\n",
      "Epoch: 1, Loss: 2.1365673542022705\n",
      "Epoch: 1, Loss: 2.470191717147827\n",
      "Epoch: 1, Loss: 1.4618442058563232\n",
      "Epoch: 1, Loss: 1.9910231828689575\n",
      "Epoch: 1, Loss: 2.6637840270996094\n",
      "Epoch: 1, Loss: 2.0953965187072754\n",
      "Epoch: 1, Loss: 1.4572224617004395\n",
      "Epoch: 1, Loss: 1.864021897315979\n",
      "Epoch: 1, Loss: 1.8756331205368042\n",
      "Epoch: 1, Loss: 1.3664531707763672\n",
      "Epoch: 1, Loss: 2.651679039001465\n",
      "Epoch: 1, Loss: 2.657844066619873\n",
      "Epoch: 1, Loss: 1.342693567276001\n",
      "Epoch: 1, Loss: 1.1723883152008057\n",
      "Epoch: 1, Loss: 1.4687049388885498\n",
      "Epoch: 1, Loss: 1.5202054977416992\n",
      "Epoch: 1, Loss: 1.2648320198059082\n",
      "Epoch: 1, Loss: 2.084125280380249\n",
      "Epoch: 1, Loss: 1.519146203994751\n",
      "Epoch: 1, Loss: 1.8433626890182495\n",
      "Epoch: 1, Loss: 1.578834891319275\n",
      "Epoch: 1, Loss: 0.8706991672515869\n",
      "Epoch: 2, Loss: 1.5253467559814453\n",
      "Epoch: 2, Loss: 1.1833583116531372\n",
      "Epoch: 2, Loss: 1.874200701713562\n",
      "Epoch: 2, Loss: 1.3955342769622803\n",
      "Epoch: 2, Loss: 0.9328435063362122\n",
      "Epoch: 2, Loss: 1.8142542839050293\n",
      "Epoch: 2, Loss: 0.7534255385398865\n",
      "Epoch: 2, Loss: 1.460981845855713\n",
      "Epoch: 2, Loss: 1.2051844596862793\n",
      "Epoch: 2, Loss: 1.9100042581558228\n",
      "Epoch: 2, Loss: 2.0229132175445557\n",
      "Epoch: 2, Loss: 2.059870958328247\n",
      "Epoch: 2, Loss: 2.3572299480438232\n",
      "Epoch: 2, Loss: 2.366882085800171\n",
      "Epoch: 2, Loss: 1.645824909210205\n",
      "Epoch: 2, Loss: 1.3490263223648071\n",
      "Epoch: 2, Loss: 2.0713095664978027\n",
      "Epoch: 2, Loss: 0.5571731328964233\n",
      "Epoch: 2, Loss: 3.183600902557373\n",
      "Epoch: 2, Loss: 3.2158493995666504\n",
      "Epoch: 2, Loss: 1.7296611070632935\n",
      "Epoch: 2, Loss: 1.7959599494934082\n",
      "Epoch: 2, Loss: 1.8801428079605103\n",
      "Epoch: 2, Loss: 1.3456517457962036\n",
      "Epoch: 2, Loss: 1.4716894626617432\n",
      "Epoch: 2, Loss: 0.9468995928764343\n",
      "Epoch: 2, Loss: 1.5336213111877441\n",
      "Epoch: 2, Loss: 1.6390118598937988\n",
      "Epoch: 2, Loss: 3.06471848487854\n",
      "Epoch: 2, Loss: 1.5909665822982788\n",
      "Epoch: 2, Loss: 1.4182920455932617\n",
      "Epoch: 2, Loss: 1.836917757987976\n",
      "Epoch: 2, Loss: 1.1559756994247437\n",
      "Epoch: 2, Loss: 2.62996244430542\n",
      "Epoch: 2, Loss: 1.8981972932815552\n",
      "Epoch: 2, Loss: 1.695777177810669\n",
      "Epoch: 2, Loss: 2.593810796737671\n",
      "Epoch: 2, Loss: 2.938955783843994\n",
      "Epoch: 2, Loss: 2.1290783882141113\n",
      "Epoch: 2, Loss: 2.8022730350494385\n",
      "Epoch: 2, Loss: 1.5288642644882202\n",
      "Epoch: 2, Loss: 1.0664678812026978\n",
      "Epoch: 2, Loss: 0.8175837993621826\n",
      "Epoch: 2, Loss: 1.107125163078308\n",
      "Epoch: 2, Loss: 2.4415676593780518\n",
      "Epoch: 2, Loss: 2.6876494884490967\n",
      "Epoch: 2, Loss: 3.1587917804718018\n",
      "Epoch: 2, Loss: 1.2240333557128906\n",
      "Epoch: 2, Loss: 1.171927809715271\n",
      "Epoch: 2, Loss: 1.4750494956970215\n",
      "Epoch: 2, Loss: 1.2338438034057617\n",
      "Epoch: 2, Loss: 2.323275089263916\n",
      "Epoch: 2, Loss: 1.8475974798202515\n",
      "Epoch: 2, Loss: 2.083789587020874\n",
      "Epoch: 2, Loss: 3.447568893432617\n",
      "Epoch: 2, Loss: 0.638831615447998\n",
      "Epoch: 2, Loss: 1.531083583831787\n",
      "Epoch: 2, Loss: 1.7300398349761963\n",
      "Epoch: 2, Loss: 2.2902231216430664\n",
      "Epoch: 2, Loss: 2.267371654510498\n",
      "Epoch: 2, Loss: 2.258211851119995\n",
      "Epoch: 2, Loss: 2.5556905269622803\n",
      "Epoch: 2, Loss: 2.0762624740600586\n",
      "Epoch: 2, Loss: 1.4034110307693481\n",
      "Epoch: 2, Loss: 2.6414968967437744\n",
      "Epoch: 2, Loss: 3.6261138916015625\n",
      "Epoch: 2, Loss: 2.679206371307373\n",
      "Epoch: 2, Loss: 1.1456868648529053\n",
      "Epoch: 2, Loss: 1.0607229471206665\n",
      "Epoch: 2, Loss: 1.9744257926940918\n",
      "Epoch: 2, Loss: 1.5213608741760254\n",
      "Epoch: 2, Loss: 1.3579291105270386\n",
      "Epoch: 2, Loss: 1.5535287857055664\n",
      "Epoch: 2, Loss: 1.9822256565093994\n",
      "Epoch: 2, Loss: 1.466651201248169\n",
      "Epoch: 2, Loss: 2.119004964828491\n",
      "Epoch: 2, Loss: 1.7938038110733032\n",
      "Epoch: 2, Loss: 1.6537574529647827\n",
      "Epoch: 2, Loss: 1.8123579025268555\n",
      "Epoch: 2, Loss: 2.302075147628784\n",
      "Epoch: 2, Loss: 1.5496853590011597\n",
      "Epoch: 2, Loss: 2.390011787414551\n",
      "Epoch: 2, Loss: 3.69464111328125\n",
      "Epoch: 2, Loss: 2.719169855117798\n",
      "Epoch: 2, Loss: 2.229987382888794\n",
      "Epoch: 2, Loss: 2.207855463027954\n",
      "Epoch: 2, Loss: 1.7951664924621582\n",
      "Epoch: 2, Loss: 1.7197728157043457\n",
      "Epoch: 2, Loss: 1.3376680612564087\n",
      "Epoch: 2, Loss: 2.1417927742004395\n",
      "Epoch: 2, Loss: 1.726332187652588\n",
      "Epoch: 2, Loss: 1.5285825729370117\n",
      "Epoch: 2, Loss: 1.1568132638931274\n",
      "Epoch: 2, Loss: 1.5387334823608398\n",
      "Epoch: 2, Loss: 1.9377162456512451\n",
      "Epoch: 2, Loss: 2.9222888946533203\n",
      "Epoch: 2, Loss: 2.7254505157470703\n",
      "Epoch: 2, Loss: 1.0500015020370483\n",
      "Epoch: 2, Loss: 2.213991641998291\n",
      "Epoch: 2, Loss: 1.275402307510376\n",
      "Epoch: 2, Loss: 1.8869045972824097\n",
      "Epoch: 2, Loss: 1.0556156635284424\n",
      "Epoch: 2, Loss: 2.186267137527466\n",
      "Epoch: 2, Loss: 1.3402475118637085\n",
      "Epoch: 2, Loss: 1.3683582544326782\n",
      "Epoch: 2, Loss: 1.5201201438903809\n",
      "Epoch: 2, Loss: 1.3534679412841797\n",
      "Epoch: 2, Loss: 1.9190638065338135\n",
      "Epoch: 2, Loss: 1.069852352142334\n",
      "Epoch: 2, Loss: 2.1751391887664795\n",
      "Epoch: 2, Loss: 2.089487075805664\n",
      "Epoch: 3, Loss: 2.1218769550323486\n",
      "Epoch: 3, Loss: 1.0953890085220337\n",
      "Epoch: 3, Loss: 3.6016743183135986\n",
      "Epoch: 3, Loss: 1.8646423816680908\n",
      "Epoch: 3, Loss: 0.9247111678123474\n",
      "Epoch: 3, Loss: 2.86087965965271\n",
      "Epoch: 3, Loss: 2.748600721359253\n",
      "Epoch: 3, Loss: 1.016556978225708\n",
      "Epoch: 3, Loss: 0.7081707715988159\n",
      "Epoch: 3, Loss: 2.943249464035034\n",
      "Epoch: 3, Loss: 2.8722035884857178\n",
      "Epoch: 3, Loss: 1.2748061418533325\n",
      "Epoch: 3, Loss: 2.296293258666992\n",
      "Epoch: 3, Loss: 2.4607129096984863\n",
      "Epoch: 3, Loss: 1.3785594701766968\n",
      "Epoch: 3, Loss: 0.5909591913223267\n",
      "Epoch: 3, Loss: 2.5066187381744385\n",
      "Epoch: 3, Loss: 2.22464919090271\n",
      "Epoch: 3, Loss: 2.089116096496582\n",
      "Epoch: 3, Loss: 1.5716638565063477\n",
      "Epoch: 3, Loss: 1.9779068231582642\n",
      "Epoch: 3, Loss: 1.455348014831543\n",
      "Epoch: 3, Loss: 1.5389317274093628\n",
      "Epoch: 3, Loss: 1.30955970287323\n",
      "Epoch: 3, Loss: 2.9048311710357666\n",
      "Epoch: 3, Loss: 2.6723806858062744\n",
      "Epoch: 3, Loss: 1.999820351600647\n",
      "Epoch: 3, Loss: 1.9459177255630493\n",
      "Epoch: 3, Loss: 2.0233142375946045\n",
      "Epoch: 3, Loss: 1.7358174324035645\n",
      "Epoch: 3, Loss: 1.3706282377243042\n",
      "Epoch: 3, Loss: 3.5715837478637695\n",
      "Epoch: 3, Loss: 0.7677614092826843\n",
      "Epoch: 3, Loss: 1.6651263236999512\n",
      "Epoch: 3, Loss: 3.4304885864257812\n",
      "Epoch: 3, Loss: 1.1631006002426147\n",
      "Epoch: 3, Loss: 2.4476256370544434\n",
      "Epoch: 3, Loss: 1.557395339012146\n",
      "Epoch: 3, Loss: 1.7991924285888672\n",
      "Epoch: 3, Loss: 1.2849805355072021\n",
      "Epoch: 3, Loss: 2.3889760971069336\n",
      "Epoch: 3, Loss: 2.01381254196167\n",
      "Epoch: 3, Loss: 2.4744954109191895\n",
      "Epoch: 3, Loss: 1.6480114459991455\n",
      "Epoch: 3, Loss: 2.3955795764923096\n",
      "Epoch: 3, Loss: 2.4739699363708496\n",
      "Epoch: 3, Loss: 1.435555338859558\n",
      "Epoch: 3, Loss: 2.5688235759735107\n",
      "Epoch: 3, Loss: 1.29021155834198\n",
      "Epoch: 3, Loss: 2.9427852630615234\n",
      "Epoch: 3, Loss: 0.9282569885253906\n",
      "Epoch: 3, Loss: 2.0581037998199463\n",
      "Epoch: 3, Loss: 1.6665440797805786\n",
      "Epoch: 3, Loss: 1.6991593837738037\n",
      "Epoch: 3, Loss: 0.6417185068130493\n",
      "Epoch: 3, Loss: 1.431469440460205\n",
      "Epoch: 3, Loss: 1.1682432889938354\n",
      "Epoch: 3, Loss: 0.9594610333442688\n",
      "Epoch: 3, Loss: 2.71028208732605\n",
      "Epoch: 3, Loss: 1.0982539653778076\n",
      "Epoch: 3, Loss: 1.5406635999679565\n",
      "Epoch: 3, Loss: 0.8684748411178589\n",
      "Epoch: 3, Loss: 3.3586835861206055\n",
      "Epoch: 3, Loss: 1.4425578117370605\n",
      "Epoch: 3, Loss: 3.007441520690918\n",
      "Epoch: 3, Loss: 1.0955939292907715\n",
      "Epoch: 3, Loss: 2.694976806640625\n",
      "Epoch: 3, Loss: 1.4381754398345947\n",
      "Epoch: 3, Loss: 2.2621212005615234\n",
      "Epoch: 3, Loss: 1.5830135345458984\n",
      "Epoch: 3, Loss: 2.4362053871154785\n",
      "Epoch: 3, Loss: 1.141469955444336\n",
      "Epoch: 3, Loss: 1.9453493356704712\n",
      "Epoch: 3, Loss: 3.042405128479004\n",
      "Epoch: 3, Loss: 2.290282726287842\n",
      "Epoch: 3, Loss: 2.723752975463867\n",
      "Epoch: 3, Loss: 1.7687675952911377\n",
      "Epoch: 3, Loss: 2.3991539478302\n",
      "Epoch: 3, Loss: 1.5420013666152954\n",
      "Epoch: 3, Loss: 1.98728346824646\n",
      "Epoch: 3, Loss: 0.6199872493743896\n",
      "Epoch: 3, Loss: 2.650023937225342\n",
      "Epoch: 3, Loss: 2.0280356407165527\n",
      "Epoch: 3, Loss: 1.7035481929779053\n",
      "Epoch: 3, Loss: 1.0766057968139648\n",
      "Epoch: 3, Loss: 0.7352094650268555\n",
      "Epoch: 3, Loss: 2.2387590408325195\n",
      "Epoch: 3, Loss: 1.3205597400665283\n",
      "Epoch: 3, Loss: 0.5480844378471375\n",
      "Epoch: 3, Loss: 0.8326691389083862\n",
      "Epoch: 3, Loss: 1.574556827545166\n",
      "Epoch: 3, Loss: 1.4026325941085815\n",
      "Epoch: 3, Loss: 1.7408052682876587\n",
      "Epoch: 3, Loss: 1.5520880222320557\n",
      "Epoch: 3, Loss: 1.4440327882766724\n",
      "Epoch: 3, Loss: 1.904789924621582\n",
      "Epoch: 3, Loss: 2.0274178981781006\n",
      "Epoch: 3, Loss: 2.3633370399475098\n",
      "Epoch: 3, Loss: 1.7481614351272583\n",
      "Epoch: 3, Loss: 1.2591028213500977\n",
      "Epoch: 3, Loss: 1.6164053678512573\n",
      "Epoch: 3, Loss: 1.3362705707550049\n",
      "Epoch: 3, Loss: 1.388120412826538\n",
      "Epoch: 3, Loss: 1.9747059345245361\n",
      "Epoch: 3, Loss: 1.661935567855835\n",
      "Epoch: 3, Loss: 0.9341862201690674\n",
      "Epoch: 3, Loss: 1.3263283967971802\n",
      "Epoch: 3, Loss: 1.5908948183059692\n",
      "Epoch: 3, Loss: 1.7908607721328735\n",
      "Epoch: 3, Loss: 1.924026608467102\n",
      "Epoch: 3, Loss: 0.7821487188339233\n",
      "Epoch: 4, Loss: 1.71713125705719\n",
      "Epoch: 4, Loss: 1.3345553874969482\n",
      "Epoch: 4, Loss: 1.3327118158340454\n",
      "Epoch: 4, Loss: 2.657259941101074\n",
      "Epoch: 4, Loss: 1.9508496522903442\n",
      "Epoch: 4, Loss: 1.5740984678268433\n",
      "Epoch: 4, Loss: 1.8863894939422607\n",
      "Epoch: 4, Loss: 1.443398118019104\n",
      "Epoch: 4, Loss: 2.8218798637390137\n",
      "Epoch: 4, Loss: 2.7947325706481934\n",
      "Epoch: 4, Loss: 2.15403151512146\n",
      "Epoch: 4, Loss: 2.4783811569213867\n",
      "Epoch: 4, Loss: 1.8895530700683594\n",
      "Epoch: 4, Loss: 0.9399468302726746\n",
      "Epoch: 4, Loss: 1.6493713855743408\n",
      "Epoch: 4, Loss: 2.3173067569732666\n",
      "Epoch: 4, Loss: 1.0470757484436035\n",
      "Epoch: 4, Loss: 2.466236114501953\n",
      "Epoch: 4, Loss: 2.0998215675354004\n",
      "Epoch: 4, Loss: 1.7887057065963745\n",
      "Epoch: 4, Loss: 1.1669446229934692\n",
      "Epoch: 4, Loss: 2.3535497188568115\n",
      "Epoch: 4, Loss: 1.1860543489456177\n",
      "Epoch: 4, Loss: 1.7596609592437744\n",
      "Epoch: 4, Loss: 2.0064926147460938\n",
      "Epoch: 4, Loss: 1.0803860425949097\n",
      "Epoch: 4, Loss: 0.5957709550857544\n",
      "Epoch: 4, Loss: 1.3379411697387695\n",
      "Epoch: 4, Loss: 2.827240467071533\n",
      "Epoch: 4, Loss: 1.1879992485046387\n",
      "Epoch: 4, Loss: 1.7934579849243164\n",
      "Epoch: 4, Loss: 1.6540180444717407\n",
      "Epoch: 4, Loss: 1.2267985343933105\n",
      "Epoch: 4, Loss: 1.443982720375061\n",
      "Epoch: 4, Loss: 1.5055251121520996\n",
      "Epoch: 4, Loss: 1.6798292398452759\n",
      "Epoch: 4, Loss: 2.6981520652770996\n",
      "Epoch: 4, Loss: 2.0277912616729736\n",
      "Epoch: 4, Loss: 2.4753940105438232\n",
      "Epoch: 4, Loss: 0.970468282699585\n",
      "Epoch: 4, Loss: 1.2429684400558472\n",
      "Epoch: 4, Loss: 1.9062728881835938\n",
      "Epoch: 4, Loss: 1.5542551279067993\n",
      "Epoch: 4, Loss: 1.1951924562454224\n",
      "Epoch: 4, Loss: 2.2027416229248047\n",
      "Epoch: 4, Loss: 2.2335596084594727\n",
      "Epoch: 4, Loss: 3.0604519844055176\n",
      "Epoch: 4, Loss: 1.9619046449661255\n",
      "Epoch: 4, Loss: 1.5350024700164795\n",
      "Epoch: 4, Loss: 1.3498278856277466\n",
      "Epoch: 4, Loss: 1.2002191543579102\n",
      "Epoch: 4, Loss: 2.6727230548858643\n",
      "Epoch: 4, Loss: 1.5492336750030518\n",
      "Epoch: 4, Loss: 1.5711535215377808\n",
      "Epoch: 4, Loss: 1.3233524560928345\n",
      "Epoch: 4, Loss: 1.9176201820373535\n",
      "Epoch: 4, Loss: 1.1294201612472534\n",
      "Epoch: 4, Loss: 2.2319176197052\n",
      "Epoch: 4, Loss: 2.019866943359375\n",
      "Epoch: 4, Loss: 2.941838264465332\n",
      "Epoch: 4, Loss: 1.6106219291687012\n",
      "Epoch: 4, Loss: 1.272865891456604\n",
      "Epoch: 4, Loss: 1.426446557044983\n",
      "Epoch: 4, Loss: 1.1821904182434082\n",
      "Epoch: 4, Loss: 1.393810510635376\n",
      "Epoch: 4, Loss: 1.9790911674499512\n",
      "Epoch: 4, Loss: 2.0540482997894287\n",
      "Epoch: 4, Loss: 2.575054407119751\n",
      "Epoch: 4, Loss: 0.6157360672950745\n",
      "Epoch: 4, Loss: 2.5091392993927\n",
      "Epoch: 4, Loss: 2.3285892009735107\n",
      "Epoch: 4, Loss: 0.9348012804985046\n",
      "Epoch: 4, Loss: 2.1090269088745117\n",
      "Epoch: 4, Loss: 1.6293855905532837\n",
      "Epoch: 4, Loss: 2.437401294708252\n",
      "Epoch: 4, Loss: 2.861567974090576\n",
      "Epoch: 4, Loss: 0.6891810297966003\n",
      "Epoch: 4, Loss: 1.7269939184188843\n",
      "Epoch: 4, Loss: 2.4056575298309326\n",
      "Epoch: 4, Loss: 2.9873955249786377\n",
      "Epoch: 4, Loss: 1.746269702911377\n",
      "Epoch: 4, Loss: 1.909135103225708\n",
      "Epoch: 4, Loss: 1.292558193206787\n",
      "Epoch: 4, Loss: 1.5469995737075806\n",
      "Epoch: 4, Loss: 1.730808973312378\n",
      "Epoch: 4, Loss: 2.043274402618408\n",
      "Epoch: 4, Loss: 2.2835042476654053\n",
      "Epoch: 4, Loss: 1.2711427211761475\n",
      "Epoch: 4, Loss: 1.6909953355789185\n",
      "Epoch: 4, Loss: 2.7308244705200195\n",
      "Epoch: 4, Loss: 1.2584067583084106\n",
      "Epoch: 4, Loss: 1.6671667098999023\n",
      "Epoch: 4, Loss: 0.7609424591064453\n",
      "Epoch: 4, Loss: 1.5054032802581787\n",
      "Epoch: 4, Loss: 0.39066681265830994\n",
      "Epoch: 4, Loss: 1.8116140365600586\n",
      "Epoch: 4, Loss: 2.7753005027770996\n",
      "Epoch: 4, Loss: 1.357851505279541\n",
      "Epoch: 4, Loss: 0.8242301940917969\n",
      "Epoch: 4, Loss: 3.3376011848449707\n",
      "Epoch: 4, Loss: 0.6094207167625427\n",
      "Epoch: 4, Loss: 0.6360265016555786\n",
      "Epoch: 4, Loss: 3.0473947525024414\n",
      "Epoch: 4, Loss: 2.2286150455474854\n",
      "Epoch: 4, Loss: 1.5479371547698975\n",
      "Epoch: 4, Loss: 1.9393489360809326\n",
      "Epoch: 4, Loss: 2.824216365814209\n",
      "Epoch: 4, Loss: 1.8771766424179077\n",
      "Epoch: 4, Loss: 2.1314501762390137\n",
      "Epoch: 4, Loss: 2.335659980773926\n",
      "Epoch: 4, Loss: 1.8538575172424316\n",
      "Training complete.\n",
      "Validation Accuracy: 0.5278\n",
      "Validation Precision: 0.3182\n",
      "Validation Recall: 0.5278\n",
      "Validation F1 Score: 0.3904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\11\\anaconda3\\envs\\wedrive\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Encode the target variable End_grid\n",
    "label_encoder = LabelEncoder()\n",
    "all_labels = list(train_data['end_grid']) + list(test_data['end_grid'])\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "train_data['end_grid_encoded'] = label_encoder.transform(train_data['end_grid'])\n",
    "test_data['end_grid_encoded'] = label_encoder.transform(test_data['end_grid'])\n",
    "\n",
    "# Extract the features and target\n",
    "train_X = train_data['start_grid'].tolist()\n",
    "train_y = train_data['end_grid_encoded'].tolist()\n",
    "test_X = test_data['start_grid'].tolist()\n",
    "test_y = test_data['end_grid_encoded'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Custom Dataset class for our data\n",
    "class GridPathDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Parameters\n",
    "MAX_LEN = 8\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = GridPathDataset(train_X, train_y, tokenizer, MAX_LEN)\n",
    "test_dataset = GridPathDataset(test_X, test_y, tokenizer, MAX_LEN)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Model training setup\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "EPOCHS = 5\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "val_targets = []\n",
    "val_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "        val_targets.extend(labels.cpu().numpy())\n",
    "        val_predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(val_targets, val_predictions)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(val_targets, val_predictions, average='weighted')\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "print(f'Validation Precision: {precision:.4f}')\n",
    "print(f'Validation Recall: {recall:.4f}')\n",
    "print(f'Validation F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파라미터 & 결과\n",
    "1. 파라미터\n",
    "MAX_LEN = 128,\n",
    "BATCH_SIZE = 16,\n",
    "EPOCHS = 3\n",
    "\n",
    "Validation Accuracy: 0.5278   \n",
    "Validation Precision: 0.3182   \n",
    "Validation Recall: 0.5278   \n",
    "Validation F1 Score: 0.3904   \n",
    "\n",
    "2. 파라미터\n",
    "MAX_LEN = 128,\n",
    "BATCH_SIZE = 8,\n",
    "EPOCHS = 5\n",
    "\n",
    "Validation Accuracy: 0.5278   \n",
    "Validation Precision: 0.3182   \n",
    "Validation Recall: 0.5278   \n",
    "Validation F1 Score: 0.3904   \n",
    "\n",
    "3. 파라미터\n",
    "MAX_LEN = 64,\n",
    "BATCH_SIZE = 8,\n",
    "EPOCHS = 3\n",
    "\n",
    "Validation Accuracy: 0.5301   \n",
    "Validation Precision: 0.3191   \n",
    "Validation Recall: 0.5301   \n",
    "Validation F1 Score: 0.3919   \n",
    "\n",
    "4. 파라미터\n",
    "MAX_LEN = 8,\n",
    "BATCH_SIZE = 8,\n",
    "EPOCHS = 3\n",
    "\n",
    "Validation Accuracy: 0.5440    \n",
    "Validation Precision: 0.3494   \n",
    "Validation Recall: 0.5440   \n",
    "Validation F1 Score: 0.4234    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wedrive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
