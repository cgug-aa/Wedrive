{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert 모델 성능 평가\n",
    "사용 데이터: od_uuid/2023/00c02071a7d249b8b528230e9d63ad1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import folium\n",
    "import geopy.distance\n",
    "\n",
    "os.chdir('../')\n",
    "data_path=os.getcwd()+'/data/od_uuid/2023'\n",
    "\n",
    "#데이터셋 칼럼명 추가\n",
    "column_name=['id','start_time','end_time','start_lat','start_lng','end_lat','end_lng','?1','?2','?3']\n",
    "\n",
    "raw_data=pd.read_csv(data_path+'/00c02071a7d249b8b528230e9d63ad1d.csv')\n",
    "data=raw_data.values.tolist()\n",
    "dataframe=pd.DataFrame(data, columns=column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test 데이터 분할\n",
    "datafilter1=dataframe['start_time'].str.contains(\"2023-10\")\n",
    "datafilter2=dataframe['start_time'].str.contains(\"2023-11\")\n",
    "datafilter3=dataframe['start_time'].str.contains(\"2023-12\")\n",
    "\n",
    "train_data=dataframe[~(datafilter1|datafilter2|datafilter3)]\n",
    "test_data=dataframe[datafilter1|datafilter2|datafilter3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리드 생성을 위한 함수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to convert numbers into corresponding letter labels\n",
    "def num_to_letter(num):\n",
    "    '''\n",
    "    num         : number that we have to convert\n",
    "    '''\n",
    "    return string.ascii_uppercase[num]\n",
    "\n",
    "def generate_initial_grids():\n",
    "        \"\"\"초기 그리드 생성\"\"\"\n",
    "        south, west, north, east = south_korea_bounds\n",
    "        lat_step = (north - south) / grid_size\n",
    "        lon_step = (east - west) / grid_size\n",
    "        grid_queue = []\n",
    "\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                grid_south = south + i * lat_step\n",
    "                grid_north = south + (i + 1) * lat_step\n",
    "                grid_west = west + j * lon_step\n",
    "                grid_east = west + (j + 1) * lon_step\n",
    "                grid_queue.append((grid_south, grid_west, grid_north, grid_east, \n",
    "                                   num_to_letter(i) + num_to_letter(j)))\n",
    "\n",
    "        return grid_queue\n",
    "\n",
    "# Create a function to get the grid label of the coordinate point\n",
    "def get_grid_label(lat, lng, final_grids):\n",
    "    '''\n",
    "    lat         : latitude\n",
    "    lng         : longitude\n",
    "    final_grids : all cells and their minimum/maximum latitude/longitude\n",
    "    '''\n",
    "    for south, west, north, east, grid_label in final_grids:\n",
    "        if south <= lat <= north and west <= lng <= east:\n",
    "            return grid_label\n",
    "    return None\n",
    "\n",
    "def is_path_in_grid(south, west, north, east, path_points):\n",
    "        \"\"\"경로가 그리드 안에 있는지 확인\"\"\"\n",
    "        return any(south <= lat <= north and west <= lng <= east for lat, lng in path_points)\n",
    "\n",
    "def subdivide_grids(grid_queue, path_points):\n",
    "    \"\"\"그리드 분할\"\"\"\n",
    "    final_grids = []\n",
    "\n",
    "    while grid_queue:\n",
    "        south, west, north, east, grid_label = grid_queue.pop(0)\n",
    "        grid_size_km = min(geopy.distance.distance((south, west), (south, east)).km,\n",
    "                           geopy.distance.distance((south, west), (north, west)).km)\n",
    "            \n",
    "        if grid_size_km > min_size_km and is_path_in_grid(south, west, north, east, path_points):\n",
    "            mid_lat = (south + north) / 2\n",
    "            mid_lon = (west + east) / 2\n",
    "            grid_queue.append((south, west, mid_lat, mid_lon, grid_label + 'C'))\n",
    "            grid_queue.append((mid_lat, west, north, mid_lon, grid_label + 'A'))\n",
    "            grid_queue.append((south, mid_lon, mid_lat, east, grid_label + 'D'))\n",
    "            grid_queue.append((mid_lat, mid_lon, north, east, grid_label + 'B'))\n",
    "        else:\n",
    "            final_grids.append((south, west, north, east, grid_label))\n",
    "\n",
    "    return final_grids\n",
    "\n",
    "# Approximate border coordinates of South Korea\n",
    "south_korea_bounds = [33.10, 124.57, 38.60, 131]\n",
    "min_size_km = 0.76\n",
    "grid_size = 13\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train 데이터 시작/종료 위치 그리드 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11\\AppData\\Local\\Temp\\ipykernel_18364\\1362932771.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['start_grid']=train_data.apply(lambda row: get_grid_label(row['start_lat'], row['start_lng'], start_final_grids), axis=1)\n",
      "C:\\Users\\11\\AppData\\Local\\Temp\\ipykernel_18364\\1362932771.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['end_grid']=train_data.apply(lambda row: get_grid_label(row['end_lat'], row['end_lng'], end_final_grids), axis=1)\n"
     ]
    }
   ],
   "source": [
    "start_points=train_data[['start_lat','start_lng']].values.tolist()\n",
    "start_grid_queue = generate_initial_grids()\n",
    "start_final_grids = subdivide_grids(start_grid_queue, start_points)\n",
    "\n",
    "end_points=train_data[['end_lat','end_lng']].values.tolist()\n",
    "end_grid_queue = generate_initial_grids()\n",
    "end_final_grids = subdivide_grids(end_grid_queue, end_points)\n",
    "\n",
    "train_data['start_grid']=train_data.apply(lambda row: get_grid_label(row['start_lat'], row['start_lng'], start_final_grids), axis=1)\n",
    "train_data['end_grid']=train_data.apply(lambda row: get_grid_label(row['end_lat'], row['end_lng'], end_final_grids), axis=1)\n",
    "train_data=train_data[['start_grid', 'end_grid']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 데이터 시작/종료 위치 그리드 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11\\AppData\\Local\\Temp\\ipykernel_18364\\3916890138.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['start_grid']=test_data.apply(lambda row: get_grid_label(row['start_lat'], row['start_lng'], start_final_grids), axis=1)\n",
      "C:\\Users\\11\\AppData\\Local\\Temp\\ipykernel_18364\\3916890138.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['end_grid']=test_data.apply(lambda row: get_grid_label(row['end_lat'], row['end_lng'], end_final_grids), axis=1)\n"
     ]
    }
   ],
   "source": [
    "start_points=test_data[['start_lat','start_lng']].values.tolist()\n",
    "start_grid_queue = generate_initial_grids()\n",
    "start_final_grids = subdivide_grids(start_grid_queue, start_points)\n",
    "\n",
    "end_points=test_data[['end_lat','end_lng']].values.tolist()\n",
    "end_grid_queue = generate_initial_grids()\n",
    "end_final_grids = subdivide_grids(end_grid_queue, end_points)\n",
    "\n",
    "test_data['start_grid']=test_data.apply(lambda row: get_grid_label(row['start_lat'], row['start_lng'], start_final_grids), axis=1)\n",
    "test_data['end_grid']=test_data.apply(lambda row: get_grid_label(row['end_lat'], row['end_lng'], end_final_grids), axis=1)\n",
    "test_data=test_data[['start_grid', 'end_grid']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_grid</th>\n",
       "      <th>end_grid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GEACAADB</td>\n",
       "      <td>EGCADDDC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EGCADDDC</td>\n",
       "      <td>GEACAADA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GEACAADA</td>\n",
       "      <td>GFACDBDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GFACDBDA</td>\n",
       "      <td>GFAADCDC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GFAADCDC</td>\n",
       "      <td>GFACDCBA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2068</th>\n",
       "      <td>GDDDDBCD</td>\n",
       "      <td>FEABACCA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2069</th>\n",
       "      <td>FEABACCA</td>\n",
       "      <td>FEACADAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2070</th>\n",
       "      <td>FEACADAB</td>\n",
       "      <td>FEACABDC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2071</th>\n",
       "      <td>FEACABDC</td>\n",
       "      <td>GDDDDDAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>GDDDDDAA</td>\n",
       "      <td>GFAADCDA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2073 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     start_grid  end_grid\n",
       "0      GEACAADB  EGCADDDC\n",
       "1      EGCADDDC  GEACAADA\n",
       "2      GEACAADA  GFACDBDA\n",
       "3      GFACDBDA  GFAADCDC\n",
       "4      GFAADCDC  GFACDCBA\n",
       "...         ...       ...\n",
       "2068   GDDDDBCD  FEABACCA\n",
       "2069   FEABACCA  FEACADAA\n",
       "2070   FEACADAB  FEACABDC\n",
       "2071   FEACABDC  GDDDDDAA\n",
       "2072   GDDDDDAA  GFAADCDA\n",
       "\n",
       "[2073 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_grid</th>\n",
       "      <th>end_grid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GEACAADB</td>\n",
       "      <td>EGCADDDC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EGCADDDC</td>\n",
       "      <td>GEACAADA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GEACAADA</td>\n",
       "      <td>GFACDBDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GFACDBDA</td>\n",
       "      <td>GFAADCDC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GFAADCDC</td>\n",
       "      <td>GFACDCBA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2604</th>\n",
       "      <td>GFACBABC</td>\n",
       "      <td>GFCADABA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2605</th>\n",
       "      <td>GFCADABA</td>\n",
       "      <td>GECDBACA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2606</th>\n",
       "      <td>GECDBACA</td>\n",
       "      <td>GECCDDAB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2607</th>\n",
       "      <td>GECDCACC</td>\n",
       "      <td>GDDDDDAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2608</th>\n",
       "      <td>GDDDDDAB</td>\n",
       "      <td>GFCABACD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2609 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     start_grid  end_grid\n",
       "0      GEACAADB  EGCADDDC\n",
       "1      EGCADDDC  GEACAADA\n",
       "2      GEACAADA  GFACDBDA\n",
       "3      GFACDBDA  GFAADCDC\n",
       "4      GFAADCDC  GFACDCBA\n",
       "...         ...       ...\n",
       "2604   GFACBABC  GFCADABA\n",
       "2605   GFCADABA  GECDBACA\n",
       "2606   GECDBACA  GECCDDAB\n",
       "2607   GECDCACC  GDDDDDAA\n",
       "2608   GDDDDDAB  GFCABACD\n",
       "\n",
       "[2609 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#전체데이터 그리드 생성\n",
    "start_points=dataframe[['start_lat','start_lng']].values.tolist()\n",
    "start_grid_queue = generate_initial_grids()\n",
    "start_final_grids = subdivide_grids(start_grid_queue, start_points)\n",
    "\n",
    "end_points=dataframe[['end_lat','end_lng']].values.tolist()\n",
    "end_grid_queue = generate_initial_grids()\n",
    "end_final_grids = subdivide_grids(end_grid_queue, end_points)\n",
    "\n",
    "dataframe['start_grid']=dataframe.apply(lambda row: get_grid_label(row['start_lat'], row['start_lng'], start_final_grids), axis=1)\n",
    "dataframe['end_grid']=dataframe.apply(lambda row: get_grid_label(row['end_lat'], row['end_lng'], end_final_grids), axis=1)\n",
    "dataframe=dataframe[['start_grid', 'end_grid']]\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert 모델 테스팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: 'GEAACDDA'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\11\\anaconda3\\envs\\wedrive\\Lib\\site-packages\\sklearn\\utils\\_encode.py:225\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_to_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\11\\anaconda3\\envs\\wedrive\\Lib\\site-packages\\sklearn\\utils\\_encode.py:165\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[1;34m(values, uniques)\u001b[0m\n\u001b[0;32m    164\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[43m[\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\11\\anaconda3\\envs\\wedrive\\Lib\\site-packages\\sklearn\\utils\\_encode.py:165\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    164\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[1;32mc:\\Users\\11\\anaconda3\\envs\\wedrive\\Lib\\site-packages\\sklearn\\utils\\_encode.py:159\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value\n\u001b[1;32m--> 159\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'GEAACDDA'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m label_encoder\u001b[38;5;241m.\u001b[39mfit(all_labels)\n\u001b[0;32m     14\u001b[0m train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_grid_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_grid\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 15\u001b[0m test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_grid_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend_grid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Extract the features and target\u001b[39;00m\n\u001b[0;32m     18\u001b[0m train_X \u001b[38;5;241m=\u001b[39m train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_grid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\11\\anaconda3\\envs\\wedrive\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:137\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\11\\anaconda3\\envs\\wedrive\\Lib\\site-packages\\sklearn\\utils\\_encode.py:227\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 227\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[1;31mValueError\u001b[0m: y contains previously unseen labels: 'GEAACDDA'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "#분기로 train/test 데이터 분할\n",
    "# Encode the target variable End_grid\n",
    "label_encoder = LabelEncoder()\n",
    "all_labels = list(train_data['end_grid']) + list(test_data['end_grid'])\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "train_data['end_grid_encoded'] = label_encoder.transform(train_data['end_grid'])\n",
    "test_data['end_grid_encoded'] = label_encoder.transform(test_data['end_grid'])\n",
    "\n",
    "# Extract the features and target\n",
    "train_X = train_data['start_grid'].tolist()\n",
    "train_y = train_data['end_grid_encoded'].tolist()\n",
    "test_X = test_data['start_grid'].tolist()\n",
    "test_y = test_data['end_grid_encoded'].tolist()\n",
    "\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Custom Dataset class for our data\n",
    "class GridPathDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Parameters\n",
    "MAX_LEN = 8\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = GridPathDataset(train_X, train_y, tokenizer, MAX_LEN)\n",
    "test_dataset = GridPathDataset(test_X, test_y, tokenizer, MAX_LEN)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Model training setup\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "EPOCHS = 5\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "val_targets = []\n",
    "val_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "        val_targets.extend(labels.cpu().numpy())\n",
    "        val_predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(val_targets, val_predictions)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(val_targets, val_predictions, average='weighted')\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "print(f'Validation Precision: {precision:.4f}')\n",
    "print(f'Validation Recall: {recall:.4f}')\n",
    "print(f'Validation F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용 데이터 000c16dad0a74e3aa088fc8616b4b220\n",
    "#### 파라미터 & 결과\n",
    "1. 파라미터\n",
    "MAX_LEN = 128,\n",
    "BATCH_SIZE = 16,\n",
    "EPOCHS = 3\n",
    "\n",
    "Validation Accuracy: 0.5278   \n",
    "Validation Precision: 0.3182   \n",
    "Validation Recall: 0.5278   \n",
    "Validation F1 Score: 0.3904   \n",
    "\n",
    "2. 파라미터\n",
    "MAX_LEN = 128,\n",
    "BATCH_SIZE = 8,\n",
    "EPOCHS = 5\n",
    "\n",
    "Validation Accuracy: 0.5278   \n",
    "Validation Precision: 0.3182   \n",
    "Validation Recall: 0.5278   \n",
    "Validation F1 Score: 0.3904   \n",
    "\n",
    "3. 파라미터\n",
    "MAX_LEN = 64,\n",
    "BATCH_SIZE = 8,\n",
    "EPOCHS = 3\n",
    "\n",
    "Validation Accuracy: 0.5301   \n",
    "Validation Precision: 0.3191   \n",
    "Validation Recall: 0.5301   \n",
    "Validation F1 Score: 0.3919   \n",
    "\n",
    "4. 파라미터\n",
    "MAX_LEN = 8,\n",
    "BATCH_SIZE = 8,\n",
    "EPOCHS = 3\n",
    "\n",
    "Validation Accuracy: 0.5440    \n",
    "Validation Precision: 0.3494   \n",
    "Validation Recall: 0.5440   \n",
    "Validation F1 Score: 0.4234    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 랜덤으로 분할한 train/test 모델 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\11\\anaconda3\\envs\\wedrive\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 5.74248743057251\n",
      "Epoch: 0, Loss: 6.080785751342773\n",
      "Epoch: 0, Loss: 5.860798358917236\n",
      "Epoch: 0, Loss: 5.956063747406006\n",
      "Epoch: 0, Loss: 5.334089756011963\n",
      "Epoch: 0, Loss: 5.955049514770508\n",
      "Epoch: 0, Loss: 5.293837547302246\n",
      "Epoch: 0, Loss: 5.496227264404297\n",
      "Epoch: 0, Loss: 5.5664448738098145\n",
      "Epoch: 0, Loss: 5.534339427947998\n",
      "Epoch: 0, Loss: 5.090738296508789\n",
      "Epoch: 0, Loss: 4.784860134124756\n",
      "Epoch: 0, Loss: 5.080949783325195\n",
      "Epoch: 0, Loss: 4.811793804168701\n",
      "Epoch: 0, Loss: 5.218809127807617\n",
      "Epoch: 0, Loss: 4.654811382293701\n",
      "Epoch: 0, Loss: 4.431087017059326\n",
      "Epoch: 0, Loss: 4.594826698303223\n",
      "Epoch: 0, Loss: 5.01944637298584\n",
      "Epoch: 0, Loss: 3.51645565032959\n",
      "Epoch: 0, Loss: 4.314260005950928\n",
      "Epoch: 0, Loss: 4.790928840637207\n",
      "Epoch: 0, Loss: 4.441497802734375\n",
      "Epoch: 0, Loss: 4.800395965576172\n",
      "Epoch: 0, Loss: 4.428159713745117\n",
      "Epoch: 0, Loss: 4.520352840423584\n",
      "Epoch: 0, Loss: 4.480280876159668\n",
      "Epoch: 0, Loss: 4.332098960876465\n",
      "Epoch: 0, Loss: 5.357780933380127\n",
      "Epoch: 0, Loss: 5.0078606605529785\n",
      "Epoch: 0, Loss: 4.797221660614014\n",
      "Epoch: 0, Loss: 3.976125955581665\n",
      "Epoch: 0, Loss: 5.157585620880127\n",
      "Epoch: 0, Loss: 3.3611998558044434\n",
      "Epoch: 0, Loss: 3.6298928260803223\n",
      "Epoch: 0, Loss: 2.7686572074890137\n",
      "Epoch: 0, Loss: 3.436188220977783\n",
      "Epoch: 0, Loss: 4.154343605041504\n",
      "Epoch: 0, Loss: 4.670212745666504\n",
      "Epoch: 0, Loss: 5.186142444610596\n",
      "Epoch: 0, Loss: 2.6322810649871826\n",
      "Epoch: 0, Loss: 3.5999209880828857\n",
      "Epoch: 0, Loss: 3.464055061340332\n",
      "Epoch: 0, Loss: 5.433011054992676\n",
      "Epoch: 0, Loss: 4.588501453399658\n",
      "Epoch: 0, Loss: 3.601458787918091\n",
      "Epoch: 0, Loss: 4.7606940269470215\n",
      "Epoch: 0, Loss: 3.9608829021453857\n",
      "Epoch: 0, Loss: 4.16995096206665\n",
      "Epoch: 0, Loss: 3.790266275405884\n",
      "Epoch: 0, Loss: 4.738045692443848\n",
      "Epoch: 0, Loss: 5.098735809326172\n",
      "Epoch: 0, Loss: 4.383389472961426\n",
      "Epoch: 0, Loss: 3.606022834777832\n",
      "Epoch: 0, Loss: 3.951463222503662\n",
      "Epoch: 0, Loss: 3.714648723602295\n",
      "Epoch: 0, Loss: 3.304306983947754\n",
      "Epoch: 0, Loss: 5.0588812828063965\n",
      "Epoch: 0, Loss: 4.60111665725708\n",
      "Epoch: 0, Loss: 5.222479343414307\n",
      "Epoch: 0, Loss: 3.789693593978882\n",
      "Epoch: 0, Loss: 4.920263290405273\n",
      "Epoch: 0, Loss: 3.9788780212402344\n",
      "Epoch: 0, Loss: 3.013364553451538\n",
      "Epoch: 0, Loss: 3.578385353088379\n",
      "Epoch: 0, Loss: 5.069921016693115\n",
      "Epoch: 0, Loss: 4.4927592277526855\n",
      "Epoch: 0, Loss: 3.9728684425354004\n",
      "Epoch: 0, Loss: 3.8060388565063477\n",
      "Epoch: 0, Loss: 6.14182186126709\n",
      "Epoch: 0, Loss: 4.439013957977295\n",
      "Epoch: 0, Loss: 3.17600679397583\n",
      "Epoch: 0, Loss: 3.357118844985962\n",
      "Epoch: 0, Loss: 4.147573471069336\n",
      "Epoch: 0, Loss: 3.966810703277588\n",
      "Epoch: 0, Loss: 4.6036834716796875\n",
      "Epoch: 0, Loss: 3.9258999824523926\n",
      "Epoch: 0, Loss: 3.75067138671875\n",
      "Epoch: 0, Loss: 4.228780746459961\n",
      "Epoch: 0, Loss: 3.5478029251098633\n",
      "Epoch: 0, Loss: 4.192997932434082\n",
      "Epoch: 0, Loss: 3.633297920227051\n",
      "Epoch: 0, Loss: 4.345166206359863\n",
      "Epoch: 0, Loss: 4.22794246673584\n",
      "Epoch: 0, Loss: 5.220081329345703\n",
      "Epoch: 0, Loss: 3.0075345039367676\n",
      "Epoch: 0, Loss: 5.0161590576171875\n",
      "Epoch: 0, Loss: 4.467001438140869\n",
      "Epoch: 0, Loss: 3.9317355155944824\n",
      "Epoch: 0, Loss: 5.640087604522705\n",
      "Epoch: 0, Loss: 3.5752482414245605\n",
      "Epoch: 0, Loss: 3.6428942680358887\n",
      "Epoch: 0, Loss: 3.0155773162841797\n",
      "Epoch: 0, Loss: 3.3142549991607666\n",
      "Epoch: 0, Loss: 4.620974540710449\n",
      "Epoch: 0, Loss: 3.164868116378784\n",
      "Epoch: 0, Loss: 4.373193740844727\n",
      "Epoch: 0, Loss: 3.1114165782928467\n",
      "Epoch: 0, Loss: 4.397136688232422\n",
      "Epoch: 0, Loss: 4.695220470428467\n",
      "Epoch: 0, Loss: 3.552649974822998\n",
      "Epoch: 0, Loss: 2.994401454925537\n",
      "Epoch: 0, Loss: 3.5037789344787598\n",
      "Epoch: 0, Loss: 4.263588905334473\n",
      "Epoch: 0, Loss: 3.2752671241760254\n",
      "Epoch: 0, Loss: 4.6782755851745605\n",
      "Epoch: 0, Loss: 4.763637542724609\n",
      "Epoch: 0, Loss: 3.8702759742736816\n",
      "Epoch: 0, Loss: 4.138066291809082\n",
      "Epoch: 0, Loss: 4.406722068786621\n",
      "Epoch: 0, Loss: 2.8744726181030273\n",
      "Epoch: 0, Loss: 3.285876750946045\n",
      "Epoch: 0, Loss: 4.124852657318115\n",
      "Epoch: 0, Loss: 4.567454814910889\n",
      "Epoch: 0, Loss: 3.451369047164917\n",
      "Epoch: 0, Loss: 3.9595513343811035\n",
      "Epoch: 0, Loss: 3.6077723503112793\n",
      "Epoch: 0, Loss: 4.14217472076416\n",
      "Epoch: 0, Loss: 3.9460039138793945\n",
      "Epoch: 0, Loss: 3.1005473136901855\n",
      "Epoch: 0, Loss: 5.019979476928711\n",
      "Epoch: 0, Loss: 5.321414947509766\n",
      "Epoch: 0, Loss: 3.7607228755950928\n",
      "Epoch: 0, Loss: 3.6604371070861816\n",
      "Epoch: 0, Loss: 5.281832218170166\n",
      "Epoch: 0, Loss: 5.252861499786377\n",
      "Epoch: 0, Loss: 5.323843479156494\n",
      "Epoch: 0, Loss: 4.153290748596191\n",
      "Epoch: 0, Loss: 4.127676486968994\n",
      "Epoch: 0, Loss: 4.504214286804199\n",
      "Epoch: 0, Loss: 4.9889421463012695\n",
      "Epoch: 0, Loss: 4.449986457824707\n",
      "Epoch: 0, Loss: 3.7883870601654053\n",
      "Epoch: 0, Loss: 3.4285054206848145\n",
      "Epoch: 0, Loss: 3.3159875869750977\n",
      "Epoch: 0, Loss: 4.005781173706055\n",
      "Epoch: 0, Loss: 4.732518196105957\n",
      "Epoch: 0, Loss: 4.12260627746582\n",
      "Epoch: 0, Loss: 1.6546787023544312\n",
      "Epoch: 0, Loss: 3.911792039871216\n",
      "Epoch: 0, Loss: 3.8691654205322266\n",
      "Epoch: 0, Loss: 3.664472818374634\n",
      "Epoch: 0, Loss: 2.790652275085449\n",
      "Epoch: 0, Loss: 4.319685935974121\n",
      "Epoch: 0, Loss: 4.06809139251709\n",
      "Epoch: 0, Loss: 5.254570007324219\n",
      "Epoch: 0, Loss: 3.284435272216797\n",
      "Epoch: 0, Loss: 4.787632942199707\n",
      "Epoch: 0, Loss: 2.5795814990997314\n",
      "Epoch: 0, Loss: 3.9043080806732178\n",
      "Epoch: 0, Loss: 4.080349922180176\n",
      "Epoch: 0, Loss: 3.85502028465271\n",
      "Epoch: 0, Loss: 5.105266571044922\n",
      "Epoch: 0, Loss: 3.6307015419006348\n",
      "Epoch: 0, Loss: 3.9785258769989014\n",
      "Epoch: 0, Loss: 2.795140266418457\n",
      "Epoch: 0, Loss: 4.225508689880371\n",
      "Epoch: 0, Loss: 3.206765651702881\n",
      "Epoch: 0, Loss: 3.799257755279541\n",
      "Epoch: 0, Loss: 5.580831050872803\n",
      "Epoch: 0, Loss: 3.128150224685669\n",
      "Epoch: 0, Loss: 2.783027172088623\n",
      "Epoch: 0, Loss: 5.916362285614014\n",
      "Epoch: 0, Loss: 4.6663899421691895\n",
      "Epoch: 0, Loss: 5.630291938781738\n",
      "Epoch: 0, Loss: 3.500540018081665\n",
      "Epoch: 0, Loss: 1.838631272315979\n",
      "Epoch: 0, Loss: 4.114373207092285\n",
      "Epoch: 0, Loss: 3.8827271461486816\n",
      "Epoch: 0, Loss: 3.039355754852295\n",
      "Epoch: 0, Loss: 3.4289212226867676\n",
      "Epoch: 0, Loss: 5.479447364807129\n",
      "Epoch: 0, Loss: 4.1444878578186035\n",
      "Epoch: 0, Loss: 3.9487385749816895\n",
      "Epoch: 0, Loss: 5.140458583831787\n",
      "Epoch: 0, Loss: 4.0957112312316895\n",
      "Epoch: 0, Loss: 2.6046950817108154\n",
      "Epoch: 0, Loss: 3.5580124855041504\n",
      "Epoch: 0, Loss: 4.510077953338623\n",
      "Epoch: 0, Loss: 4.902254581451416\n",
      "Epoch: 0, Loss: 5.210265159606934\n",
      "Epoch: 0, Loss: 3.956655263900757\n",
      "Epoch: 0, Loss: 3.1988813877105713\n",
      "Epoch: 0, Loss: 3.4955801963806152\n",
      "Epoch: 0, Loss: 3.151707172393799\n",
      "Epoch: 0, Loss: 3.74053955078125\n",
      "Epoch: 0, Loss: 2.116826295852661\n",
      "Epoch: 0, Loss: 4.510546684265137\n",
      "Epoch: 0, Loss: 3.1892991065979004\n",
      "Epoch: 0, Loss: 4.154913902282715\n",
      "Epoch: 0, Loss: 3.389914035797119\n",
      "Epoch: 0, Loss: 4.1513190269470215\n",
      "Epoch: 0, Loss: 3.277885913848877\n",
      "Epoch: 0, Loss: 6.0024566650390625\n",
      "Epoch: 0, Loss: 2.8584299087524414\n",
      "Epoch: 0, Loss: 5.106890678405762\n",
      "Epoch: 0, Loss: 3.98952317237854\n",
      "Epoch: 0, Loss: 3.2671642303466797\n",
      "Epoch: 0, Loss: 2.847801446914673\n",
      "Epoch: 0, Loss: 3.9368319511413574\n",
      "Epoch: 0, Loss: 4.090537071228027\n",
      "Epoch: 0, Loss: 3.3422415256500244\n",
      "Epoch: 0, Loss: 3.42659592628479\n",
      "Epoch: 0, Loss: 3.760883092880249\n",
      "Epoch: 0, Loss: 3.1590042114257812\n",
      "Epoch: 0, Loss: 2.5422627925872803\n",
      "Epoch: 0, Loss: 2.3330276012420654\n",
      "Epoch: 0, Loss: 2.9868311882019043\n",
      "Epoch: 0, Loss: 4.48333740234375\n",
      "Epoch: 0, Loss: 4.999374866485596\n",
      "Epoch: 0, Loss: 4.74677038192749\n",
      "Epoch: 0, Loss: 4.7825775146484375\n",
      "Epoch: 0, Loss: 2.5364933013916016\n",
      "Epoch: 0, Loss: 2.0513556003570557\n",
      "Epoch: 0, Loss: 5.219147205352783\n",
      "Epoch: 0, Loss: 4.559549808502197\n",
      "Epoch: 0, Loss: 5.002917289733887\n",
      "Epoch: 0, Loss: 2.6738100051879883\n",
      "Epoch: 0, Loss: 3.7903339862823486\n",
      "Epoch: 0, Loss: 4.619202613830566\n",
      "Epoch: 0, Loss: 3.74594783782959\n",
      "Epoch: 0, Loss: 3.982206344604492\n",
      "Epoch: 0, Loss: 4.804255485534668\n",
      "Epoch: 0, Loss: 3.246842622756958\n",
      "Epoch: 0, Loss: 5.895529747009277\n",
      "Epoch: 0, Loss: 3.1372175216674805\n",
      "Epoch: 0, Loss: 3.086557388305664\n",
      "Epoch: 0, Loss: 4.596770286560059\n",
      "Epoch: 0, Loss: 3.3757457733154297\n",
      "Epoch: 0, Loss: 2.566490650177002\n",
      "Epoch: 0, Loss: 4.8274922370910645\n",
      "Epoch: 0, Loss: 2.8885438442230225\n",
      "Epoch: 0, Loss: 5.341308116912842\n",
      "Epoch: 0, Loss: 2.418034315109253\n",
      "Epoch: 0, Loss: 3.915391445159912\n",
      "Epoch: 0, Loss: 3.266709327697754\n",
      "Epoch: 0, Loss: 3.8956499099731445\n",
      "Epoch: 0, Loss: 4.975499629974365\n",
      "Epoch: 0, Loss: 3.48531436920166\n",
      "Epoch: 0, Loss: 3.595668077468872\n",
      "Epoch: 0, Loss: 3.257573366165161\n",
      "Epoch: 0, Loss: 3.3051087856292725\n",
      "Epoch: 0, Loss: 3.796304225921631\n",
      "Epoch: 0, Loss: 3.8129758834838867\n",
      "Epoch: 0, Loss: 4.710649013519287\n",
      "Epoch: 0, Loss: 3.0443930625915527\n",
      "Epoch: 0, Loss: 5.46208381652832\n",
      "Epoch: 0, Loss: 4.625779628753662\n",
      "Epoch: 0, Loss: 2.1639244556427\n",
      "Epoch: 0, Loss: 3.2041540145874023\n",
      "Epoch: 0, Loss: 5.286380290985107\n",
      "Epoch: 0, Loss: 2.6344597339630127\n",
      "Epoch: 0, Loss: 3.4678142070770264\n",
      "Epoch: 0, Loss: 4.637807846069336\n",
      "Epoch: 0, Loss: 3.2042787075042725\n",
      "Epoch: 0, Loss: 2.7031781673431396\n",
      "Epoch: 0, Loss: 2.9331912994384766\n",
      "Epoch: 0, Loss: 3.6592559814453125\n",
      "Epoch: 0, Loss: 5.161205768585205\n",
      "Epoch: 0, Loss: 4.10343599319458\n",
      "Epoch: 0, Loss: 5.253344535827637\n",
      "Epoch: 1, Loss: 3.780935525894165\n",
      "Epoch: 1, Loss: 2.518035411834717\n",
      "Epoch: 1, Loss: 4.675868988037109\n",
      "Epoch: 1, Loss: 4.52058219909668\n",
      "Epoch: 1, Loss: 3.2342629432678223\n",
      "Epoch: 1, Loss: 2.911496162414551\n",
      "Epoch: 1, Loss: 3.4414222240448\n",
      "Epoch: 1, Loss: 4.056561470031738\n",
      "Epoch: 1, Loss: 2.5014350414276123\n",
      "Epoch: 1, Loss: 4.185173034667969\n",
      "Epoch: 1, Loss: 2.4593489170074463\n",
      "Epoch: 1, Loss: 3.4193577766418457\n",
      "Epoch: 1, Loss: 3.476759672164917\n",
      "Epoch: 1, Loss: 4.930248737335205\n",
      "Epoch: 1, Loss: 4.053823471069336\n",
      "Epoch: 1, Loss: 3.7335150241851807\n",
      "Epoch: 1, Loss: 3.2197413444519043\n",
      "Epoch: 1, Loss: 3.7532613277435303\n",
      "Epoch: 1, Loss: 3.7644197940826416\n",
      "Epoch: 1, Loss: 2.876429557800293\n",
      "Epoch: 1, Loss: 3.494123697280884\n",
      "Epoch: 1, Loss: 2.6418533325195312\n",
      "Epoch: 1, Loss: 4.571169376373291\n",
      "Epoch: 1, Loss: 4.378904342651367\n",
      "Epoch: 1, Loss: 4.113867282867432\n",
      "Epoch: 1, Loss: 4.636194229125977\n",
      "Epoch: 1, Loss: 3.5182416439056396\n",
      "Epoch: 1, Loss: 4.090943336486816\n",
      "Epoch: 1, Loss: 4.143063068389893\n",
      "Epoch: 1, Loss: 3.673950433731079\n",
      "Epoch: 1, Loss: 3.1859991550445557\n",
      "Epoch: 1, Loss: 2.6311421394348145\n",
      "Epoch: 1, Loss: 3.6269986629486084\n",
      "Epoch: 1, Loss: 4.671849727630615\n",
      "Epoch: 1, Loss: 2.9568490982055664\n",
      "Epoch: 1, Loss: 2.240269184112549\n",
      "Epoch: 1, Loss: 3.49151873588562\n",
      "Epoch: 1, Loss: 2.9103517532348633\n",
      "Epoch: 1, Loss: 4.048624515533447\n",
      "Epoch: 1, Loss: 2.935441732406616\n",
      "Epoch: 1, Loss: 4.5828680992126465\n",
      "Epoch: 1, Loss: 3.171781539916992\n",
      "Epoch: 1, Loss: 4.662284851074219\n",
      "Epoch: 1, Loss: 2.790539026260376\n",
      "Epoch: 1, Loss: 2.175682783126831\n",
      "Epoch: 1, Loss: 3.866447687149048\n",
      "Epoch: 1, Loss: 3.530848503112793\n",
      "Epoch: 1, Loss: 3.165858745574951\n",
      "Epoch: 1, Loss: 3.91058087348938\n",
      "Epoch: 1, Loss: 2.935365915298462\n",
      "Epoch: 1, Loss: 3.4229378700256348\n",
      "Epoch: 1, Loss: 2.6527657508850098\n",
      "Epoch: 1, Loss: 4.354625225067139\n",
      "Epoch: 1, Loss: 3.5511889457702637\n",
      "Epoch: 1, Loss: 4.660436630249023\n",
      "Epoch: 1, Loss: 2.9613800048828125\n",
      "Epoch: 1, Loss: 3.999192714691162\n",
      "Epoch: 1, Loss: 2.722325325012207\n",
      "Epoch: 1, Loss: 3.978382110595703\n",
      "Epoch: 1, Loss: 3.971245527267456\n",
      "Epoch: 1, Loss: 3.338865041732788\n",
      "Epoch: 1, Loss: 3.7069358825683594\n",
      "Epoch: 1, Loss: 4.108416557312012\n",
      "Epoch: 1, Loss: 2.1161370277404785\n",
      "Epoch: 1, Loss: 4.201136589050293\n",
      "Epoch: 1, Loss: 3.607367992401123\n",
      "Epoch: 1, Loss: 2.3174455165863037\n",
      "Epoch: 1, Loss: 2.4491562843322754\n",
      "Epoch: 1, Loss: 4.075361251831055\n",
      "Epoch: 1, Loss: 4.169751167297363\n",
      "Epoch: 1, Loss: 3.8298468589782715\n",
      "Epoch: 1, Loss: 2.8824687004089355\n",
      "Epoch: 1, Loss: 3.808152675628662\n",
      "Epoch: 1, Loss: 3.7821922302246094\n",
      "Epoch: 1, Loss: 3.8722126483917236\n",
      "Epoch: 1, Loss: 3.9324700832366943\n",
      "Epoch: 1, Loss: 3.0703251361846924\n",
      "Epoch: 1, Loss: 3.625699996948242\n",
      "Epoch: 1, Loss: 2.8923637866973877\n",
      "Epoch: 1, Loss: 4.228156089782715\n",
      "Epoch: 1, Loss: 3.924126148223877\n",
      "Epoch: 1, Loss: 3.7862722873687744\n",
      "Epoch: 1, Loss: 4.153916358947754\n",
      "Epoch: 1, Loss: 3.203676700592041\n",
      "Epoch: 1, Loss: 3.923698902130127\n",
      "Epoch: 1, Loss: 3.054316520690918\n",
      "Epoch: 1, Loss: 2.597363233566284\n",
      "Epoch: 1, Loss: 4.579034328460693\n",
      "Epoch: 1, Loss: 2.7429559230804443\n",
      "Epoch: 1, Loss: 1.6484366655349731\n",
      "Epoch: 1, Loss: 2.241379737854004\n",
      "Epoch: 1, Loss: 3.7849953174591064\n",
      "Epoch: 1, Loss: 3.4986472129821777\n",
      "Epoch: 1, Loss: 4.31707239151001\n",
      "Epoch: 1, Loss: 2.3858230113983154\n",
      "Epoch: 1, Loss: 2.1885571479797363\n",
      "Epoch: 1, Loss: 2.675905227661133\n",
      "Epoch: 1, Loss: 2.991889476776123\n",
      "Epoch: 1, Loss: 4.019069194793701\n",
      "Epoch: 1, Loss: 4.536251544952393\n",
      "Epoch: 1, Loss: 3.7037336826324463\n",
      "Epoch: 1, Loss: 1.9336423873901367\n",
      "Epoch: 1, Loss: 4.0202717781066895\n",
      "Epoch: 1, Loss: 2.7314612865448\n",
      "Epoch: 1, Loss: 2.9980533123016357\n",
      "Epoch: 1, Loss: 2.814483165740967\n",
      "Epoch: 1, Loss: 3.433919906616211\n",
      "Epoch: 1, Loss: 5.058826446533203\n",
      "Epoch: 1, Loss: 4.0953264236450195\n",
      "Epoch: 1, Loss: 5.437983989715576\n",
      "Epoch: 1, Loss: 5.127838611602783\n",
      "Epoch: 1, Loss: 4.533149719238281\n",
      "Epoch: 1, Loss: 2.381493330001831\n",
      "Epoch: 1, Loss: 3.7486352920532227\n",
      "Epoch: 1, Loss: 4.086497783660889\n",
      "Epoch: 1, Loss: 4.042032718658447\n",
      "Epoch: 1, Loss: 3.565915822982788\n",
      "Epoch: 1, Loss: 5.658047199249268\n",
      "Epoch: 1, Loss: 3.706239700317383\n",
      "Epoch: 1, Loss: 3.478039026260376\n",
      "Epoch: 1, Loss: 2.574373960494995\n",
      "Epoch: 1, Loss: 4.0650634765625\n",
      "Epoch: 1, Loss: 3.8507843017578125\n",
      "Epoch: 1, Loss: 3.290666103363037\n",
      "Epoch: 1, Loss: 4.20821475982666\n",
      "Epoch: 1, Loss: 2.8190722465515137\n",
      "Epoch: 1, Loss: 3.8986146450042725\n",
      "Epoch: 1, Loss: 3.79142427444458\n",
      "Epoch: 1, Loss: 3.4573798179626465\n",
      "Epoch: 1, Loss: 3.4093880653381348\n",
      "Epoch: 1, Loss: 3.7564120292663574\n",
      "Epoch: 1, Loss: 3.1755824089050293\n",
      "Epoch: 1, Loss: 5.035661220550537\n",
      "Epoch: 1, Loss: 3.743101119995117\n",
      "Epoch: 1, Loss: 4.293570518493652\n",
      "Epoch: 1, Loss: 3.673053503036499\n",
      "Epoch: 1, Loss: 3.777973175048828\n",
      "Epoch: 1, Loss: 4.640077114105225\n",
      "Epoch: 1, Loss: 3.6401174068450928\n",
      "Epoch: 1, Loss: 3.650872230529785\n",
      "Epoch: 1, Loss: 3.2250280380249023\n",
      "Epoch: 1, Loss: 2.392758846282959\n",
      "Epoch: 1, Loss: 3.236018180847168\n",
      "Epoch: 1, Loss: 4.666444301605225\n",
      "Epoch: 1, Loss: 2.3781027793884277\n",
      "Epoch: 1, Loss: 4.742491245269775\n",
      "Epoch: 1, Loss: 4.347555160522461\n",
      "Epoch: 1, Loss: 4.953145503997803\n",
      "Epoch: 1, Loss: 5.271551132202148\n",
      "Epoch: 1, Loss: 3.5054147243499756\n",
      "Epoch: 1, Loss: 3.4324867725372314\n",
      "Epoch: 1, Loss: 3.632506847381592\n",
      "Epoch: 1, Loss: 2.854189157485962\n",
      "Epoch: 1, Loss: 3.9091055393218994\n",
      "Epoch: 1, Loss: 4.094323635101318\n",
      "Epoch: 1, Loss: 2.7510733604431152\n",
      "Epoch: 1, Loss: 3.394414186477661\n",
      "Epoch: 1, Loss: 4.8730034828186035\n",
      "Epoch: 1, Loss: 1.9379026889801025\n",
      "Epoch: 1, Loss: 3.9962100982666016\n",
      "Epoch: 1, Loss: 3.3461735248565674\n",
      "Epoch: 1, Loss: 3.0541231632232666\n",
      "Epoch: 1, Loss: 3.7841012477874756\n",
      "Epoch: 1, Loss: 3.5563101768493652\n",
      "Epoch: 1, Loss: 4.455385208129883\n",
      "Epoch: 1, Loss: 1.9276686906814575\n",
      "Epoch: 1, Loss: 3.5569002628326416\n",
      "Epoch: 1, Loss: 3.5277557373046875\n",
      "Epoch: 1, Loss: 3.3567135334014893\n",
      "Epoch: 1, Loss: 4.9449663162231445\n",
      "Epoch: 1, Loss: 4.101119041442871\n",
      "Epoch: 1, Loss: 2.429332971572876\n",
      "Epoch: 1, Loss: 1.6903597116470337\n",
      "Epoch: 1, Loss: 3.754441499710083\n",
      "Epoch: 1, Loss: 3.0324792861938477\n",
      "Epoch: 1, Loss: 3.5359015464782715\n",
      "Epoch: 1, Loss: 4.763110637664795\n",
      "Epoch: 1, Loss: 3.7568299770355225\n",
      "Epoch: 1, Loss: 3.2027435302734375\n",
      "Epoch: 1, Loss: 3.4911415576934814\n",
      "Epoch: 1, Loss: 5.384762763977051\n",
      "Epoch: 1, Loss: 3.6278488636016846\n",
      "Epoch: 1, Loss: 3.044262409210205\n",
      "Epoch: 1, Loss: 4.992522239685059\n",
      "Epoch: 1, Loss: 4.228275775909424\n",
      "Epoch: 1, Loss: 3.70460844039917\n",
      "Epoch: 1, Loss: 3.4417378902435303\n",
      "Epoch: 1, Loss: 3.942134380340576\n",
      "Epoch: 1, Loss: 3.3529536724090576\n",
      "Epoch: 1, Loss: 3.567601203918457\n",
      "Epoch: 1, Loss: 3.1733102798461914\n",
      "Epoch: 1, Loss: 4.875707149505615\n",
      "Epoch: 1, Loss: 3.730248212814331\n",
      "Epoch: 1, Loss: 2.3846161365509033\n",
      "Epoch: 1, Loss: 3.5396013259887695\n",
      "Epoch: 1, Loss: 3.70249080657959\n",
      "Epoch: 1, Loss: 2.556898832321167\n",
      "Epoch: 1, Loss: 2.837801933288574\n",
      "Epoch: 1, Loss: 2.8098108768463135\n",
      "Epoch: 1, Loss: 4.355316162109375\n",
      "Epoch: 1, Loss: 3.2632007598876953\n",
      "Epoch: 1, Loss: 2.842881679534912\n",
      "Epoch: 1, Loss: 4.075871467590332\n",
      "Epoch: 1, Loss: 3.0014846324920654\n",
      "Epoch: 1, Loss: 2.7434873580932617\n",
      "Epoch: 1, Loss: 3.4211554527282715\n",
      "Epoch: 1, Loss: 3.3232004642486572\n",
      "Epoch: 1, Loss: 3.7411088943481445\n",
      "Epoch: 1, Loss: 4.363705635070801\n",
      "Epoch: 1, Loss: 2.8013699054718018\n",
      "Epoch: 1, Loss: 3.7223286628723145\n",
      "Epoch: 1, Loss: 3.3146450519561768\n",
      "Epoch: 1, Loss: 2.3510959148406982\n",
      "Epoch: 1, Loss: 4.679626941680908\n",
      "Epoch: 1, Loss: 3.89848256111145\n",
      "Epoch: 1, Loss: 4.50363826751709\n",
      "Epoch: 1, Loss: 2.629396915435791\n",
      "Epoch: 1, Loss: 2.3008086681365967\n",
      "Epoch: 1, Loss: 3.562764883041382\n",
      "Epoch: 1, Loss: 3.627072811126709\n",
      "Epoch: 1, Loss: 3.111191749572754\n",
      "Epoch: 1, Loss: 2.5884671211242676\n",
      "Epoch: 1, Loss: 2.961554765701294\n",
      "Epoch: 1, Loss: 3.9487059116363525\n",
      "Epoch: 1, Loss: 4.6064276695251465\n",
      "Epoch: 1, Loss: 3.812729597091675\n",
      "Epoch: 1, Loss: 3.9036355018615723\n",
      "Epoch: 1, Loss: 3.904609441757202\n",
      "Epoch: 1, Loss: 3.7176012992858887\n",
      "Epoch: 1, Loss: 3.051217794418335\n",
      "Epoch: 1, Loss: 4.231727600097656\n",
      "Epoch: 1, Loss: 3.6586225032806396\n",
      "Epoch: 1, Loss: 5.174997329711914\n",
      "Epoch: 1, Loss: 3.474822759628296\n",
      "Epoch: 1, Loss: 3.4572510719299316\n",
      "Epoch: 1, Loss: 3.481682062149048\n",
      "Epoch: 1, Loss: 3.987470865249634\n",
      "Epoch: 1, Loss: 4.928930759429932\n",
      "Epoch: 1, Loss: 4.324335098266602\n",
      "Epoch: 1, Loss: 3.223759889602661\n",
      "Epoch: 1, Loss: 2.9988653659820557\n",
      "Epoch: 1, Loss: 5.060664653778076\n",
      "Epoch: 1, Loss: 4.413759231567383\n",
      "Epoch: 1, Loss: 3.838332414627075\n",
      "Epoch: 1, Loss: 3.630101203918457\n",
      "Epoch: 1, Loss: 3.766357660293579\n",
      "Epoch: 1, Loss: 2.89499568939209\n",
      "Epoch: 1, Loss: 3.605923652648926\n",
      "Epoch: 1, Loss: 4.376382827758789\n",
      "Epoch: 1, Loss: 4.414392948150635\n",
      "Epoch: 1, Loss: 4.1044840812683105\n",
      "Epoch: 1, Loss: 3.910276412963867\n",
      "Epoch: 1, Loss: 2.721266746520996\n",
      "Epoch: 1, Loss: 3.4117283821105957\n",
      "Epoch: 1, Loss: 3.8703861236572266\n",
      "Epoch: 1, Loss: 3.868764877319336\n",
      "Epoch: 1, Loss: 4.140005111694336\n",
      "Epoch: 1, Loss: 5.204631328582764\n",
      "Epoch: 1, Loss: 6.10896635055542\n",
      "Epoch: 1, Loss: 3.552903652191162\n",
      "Epoch: 1, Loss: 3.0602915287017822\n",
      "Epoch: 2, Loss: 4.256162166595459\n",
      "Epoch: 2, Loss: 3.382488489151001\n",
      "Epoch: 2, Loss: 4.354644775390625\n",
      "Epoch: 2, Loss: 2.905850887298584\n",
      "Epoch: 2, Loss: 2.854635238647461\n",
      "Epoch: 2, Loss: 4.001391410827637\n",
      "Epoch: 2, Loss: 4.065671920776367\n",
      "Epoch: 2, Loss: 3.4869351387023926\n",
      "Epoch: 2, Loss: 2.8393280506134033\n",
      "Epoch: 2, Loss: 3.458008050918579\n",
      "Epoch: 2, Loss: 4.405826568603516\n",
      "Epoch: 2, Loss: 3.025397539138794\n",
      "Epoch: 2, Loss: 3.4109249114990234\n",
      "Epoch: 2, Loss: 4.0389227867126465\n",
      "Epoch: 2, Loss: 3.5730671882629395\n",
      "Epoch: 2, Loss: 2.694373369216919\n",
      "Epoch: 2, Loss: 2.5643060207366943\n",
      "Epoch: 2, Loss: 1.699955940246582\n",
      "Epoch: 2, Loss: 3.270930290222168\n",
      "Epoch: 2, Loss: 3.8827931880950928\n",
      "Epoch: 2, Loss: 2.6619439125061035\n",
      "Epoch: 2, Loss: 3.381470203399658\n",
      "Epoch: 2, Loss: 3.3932173252105713\n",
      "Epoch: 2, Loss: 3.5441508293151855\n",
      "Epoch: 2, Loss: 4.63847541809082\n",
      "Epoch: 2, Loss: 3.3425135612487793\n",
      "Epoch: 2, Loss: 3.4295461177825928\n",
      "Epoch: 2, Loss: 2.60306978225708\n",
      "Epoch: 2, Loss: 2.092428207397461\n",
      "Epoch: 2, Loss: 3.2903733253479004\n",
      "Epoch: 2, Loss: 4.2459025382995605\n",
      "Epoch: 2, Loss: 2.7908647060394287\n",
      "Epoch: 2, Loss: 3.8633222579956055\n",
      "Epoch: 2, Loss: 2.408447742462158\n",
      "Epoch: 2, Loss: 1.572327733039856\n",
      "Epoch: 2, Loss: 5.145470142364502\n",
      "Epoch: 2, Loss: 2.515408754348755\n",
      "Epoch: 2, Loss: 3.6480722427368164\n",
      "Epoch: 2, Loss: 5.595869064331055\n",
      "Epoch: 2, Loss: 4.154231071472168\n",
      "Epoch: 2, Loss: 3.1342079639434814\n",
      "Epoch: 2, Loss: 4.502469062805176\n",
      "Epoch: 2, Loss: 1.3783087730407715\n",
      "Epoch: 2, Loss: 3.137667417526245\n",
      "Epoch: 2, Loss: 3.4015231132507324\n",
      "Epoch: 2, Loss: 2.674060344696045\n",
      "Epoch: 2, Loss: 1.7729288339614868\n",
      "Epoch: 2, Loss: 1.4705246686935425\n",
      "Epoch: 2, Loss: 4.675060272216797\n",
      "Epoch: 2, Loss: 3.2388205528259277\n",
      "Epoch: 2, Loss: 3.853543758392334\n",
      "Epoch: 2, Loss: 4.445723056793213\n",
      "Epoch: 2, Loss: 5.135433673858643\n",
      "Epoch: 2, Loss: 3.9683926105499268\n",
      "Epoch: 2, Loss: 2.3807592391967773\n",
      "Epoch: 2, Loss: 2.969493865966797\n",
      "Epoch: 2, Loss: 3.5146491527557373\n",
      "Epoch: 2, Loss: 3.4834909439086914\n",
      "Epoch: 2, Loss: 4.23212194442749\n",
      "Epoch: 2, Loss: 3.759158134460449\n",
      "Epoch: 2, Loss: 2.5678157806396484\n",
      "Epoch: 2, Loss: 3.8891963958740234\n",
      "Epoch: 2, Loss: 4.631809711456299\n",
      "Epoch: 2, Loss: 4.764286041259766\n",
      "Epoch: 2, Loss: 2.5813796520233154\n",
      "Epoch: 2, Loss: 4.507184982299805\n",
      "Epoch: 2, Loss: 3.810147762298584\n",
      "Epoch: 2, Loss: 3.909752607345581\n",
      "Epoch: 2, Loss: 3.0241682529449463\n",
      "Epoch: 2, Loss: 4.011611461639404\n",
      "Epoch: 2, Loss: 2.140331268310547\n",
      "Epoch: 2, Loss: 3.087970018386841\n",
      "Epoch: 2, Loss: 3.64452862739563\n",
      "Epoch: 2, Loss: 2.696817636489868\n",
      "Epoch: 2, Loss: 3.28851318359375\n",
      "Epoch: 2, Loss: 3.2558624744415283\n",
      "Epoch: 2, Loss: 2.980499505996704\n",
      "Epoch: 2, Loss: 4.116184234619141\n",
      "Epoch: 2, Loss: 4.0783586502075195\n",
      "Epoch: 2, Loss: 3.8930087089538574\n",
      "Epoch: 2, Loss: 4.263424873352051\n",
      "Epoch: 2, Loss: 4.212656497955322\n",
      "Epoch: 2, Loss: 4.371858596801758\n",
      "Epoch: 2, Loss: 1.4865704774856567\n",
      "Epoch: 2, Loss: 4.5703229904174805\n",
      "Epoch: 2, Loss: 3.6519792079925537\n",
      "Epoch: 2, Loss: 3.22200345993042\n",
      "Epoch: 2, Loss: 3.7966935634613037\n",
      "Epoch: 2, Loss: 3.194162130355835\n",
      "Epoch: 2, Loss: 3.625178337097168\n",
      "Epoch: 2, Loss: 3.452376127243042\n",
      "Epoch: 2, Loss: 2.4841063022613525\n",
      "Epoch: 2, Loss: 3.248718023300171\n",
      "Epoch: 2, Loss: 1.8075648546218872\n",
      "Epoch: 2, Loss: 2.199223756790161\n",
      "Epoch: 2, Loss: 2.690610647201538\n",
      "Epoch: 2, Loss: 3.4449498653411865\n",
      "Epoch: 2, Loss: 4.750296592712402\n",
      "Epoch: 2, Loss: 4.473635673522949\n",
      "Epoch: 2, Loss: 4.094764709472656\n",
      "Epoch: 2, Loss: 3.721405267715454\n",
      "Epoch: 2, Loss: 3.8223838806152344\n",
      "Epoch: 2, Loss: 3.2394063472747803\n",
      "Epoch: 2, Loss: 3.2154479026794434\n",
      "Epoch: 2, Loss: 3.2868170738220215\n",
      "Epoch: 2, Loss: 2.325939178466797\n",
      "Epoch: 2, Loss: 3.593937635421753\n",
      "Epoch: 2, Loss: 2.9550819396972656\n",
      "Epoch: 2, Loss: 4.684332370758057\n",
      "Epoch: 2, Loss: 2.1212544441223145\n",
      "Epoch: 2, Loss: 3.8995397090911865\n",
      "Epoch: 2, Loss: 4.2994184494018555\n",
      "Epoch: 2, Loss: 5.008387565612793\n",
      "Epoch: 2, Loss: 2.625502347946167\n",
      "Epoch: 2, Loss: 4.285961151123047\n",
      "Epoch: 2, Loss: 3.528813600540161\n",
      "Epoch: 2, Loss: 3.887200117111206\n",
      "Epoch: 2, Loss: 3.7322838306427\n",
      "Epoch: 2, Loss: 2.8627662658691406\n",
      "Epoch: 2, Loss: 3.639126777648926\n",
      "Epoch: 2, Loss: 4.080097198486328\n",
      "Epoch: 2, Loss: 3.071049690246582\n",
      "Epoch: 2, Loss: 2.6624844074249268\n",
      "Epoch: 2, Loss: 3.6375081539154053\n",
      "Epoch: 2, Loss: 4.277320384979248\n",
      "Epoch: 2, Loss: 2.763237237930298\n",
      "Epoch: 2, Loss: 4.91882848739624\n",
      "Epoch: 2, Loss: 3.132166624069214\n",
      "Epoch: 2, Loss: 4.797460556030273\n",
      "Epoch: 2, Loss: 3.5933878421783447\n",
      "Epoch: 2, Loss: 4.349205017089844\n",
      "Epoch: 2, Loss: 3.220839500427246\n",
      "Epoch: 2, Loss: 3.2036852836608887\n",
      "Epoch: 2, Loss: 4.040658473968506\n",
      "Epoch: 2, Loss: 2.8511528968811035\n",
      "Epoch: 2, Loss: 3.910431146621704\n",
      "Epoch: 2, Loss: 3.5701966285705566\n",
      "Epoch: 2, Loss: 3.3665313720703125\n",
      "Epoch: 2, Loss: 3.1424155235290527\n",
      "Epoch: 2, Loss: 4.716803073883057\n",
      "Epoch: 2, Loss: 3.2600061893463135\n",
      "Epoch: 2, Loss: 3.99562931060791\n",
      "Epoch: 2, Loss: 2.5442967414855957\n",
      "Epoch: 2, Loss: 3.8353724479675293\n",
      "Epoch: 2, Loss: 4.355939865112305\n",
      "Epoch: 2, Loss: 2.6642282009124756\n",
      "Epoch: 2, Loss: 2.909379482269287\n",
      "Epoch: 2, Loss: 2.782923460006714\n",
      "Epoch: 2, Loss: 3.373981475830078\n",
      "Epoch: 2, Loss: 2.68432354927063\n",
      "Epoch: 2, Loss: 3.9585323333740234\n",
      "Epoch: 2, Loss: 3.7796788215637207\n",
      "Epoch: 2, Loss: 4.695631504058838\n",
      "Epoch: 2, Loss: 2.8499319553375244\n",
      "Epoch: 2, Loss: 3.3882200717926025\n",
      "Epoch: 2, Loss: 3.905363082885742\n",
      "Epoch: 2, Loss: 4.404443740844727\n",
      "Epoch: 2, Loss: 3.6065587997436523\n",
      "Epoch: 2, Loss: 2.8915274143218994\n",
      "Epoch: 2, Loss: 3.15866756439209\n",
      "Epoch: 2, Loss: 4.357979774475098\n",
      "Epoch: 2, Loss: 3.305427074432373\n",
      "Epoch: 2, Loss: 2.629335641860962\n",
      "Epoch: 2, Loss: 3.700611114501953\n",
      "Epoch: 2, Loss: 3.2187421321868896\n",
      "Epoch: 2, Loss: 2.5759267807006836\n",
      "Epoch: 2, Loss: 2.503770112991333\n",
      "Epoch: 2, Loss: 2.508542060852051\n",
      "Epoch: 2, Loss: 2.910714626312256\n",
      "Epoch: 2, Loss: 2.901024341583252\n",
      "Epoch: 2, Loss: 2.899123191833496\n",
      "Epoch: 2, Loss: 2.659956455230713\n",
      "Epoch: 2, Loss: 2.828185796737671\n",
      "Epoch: 2, Loss: 4.109489440917969\n",
      "Epoch: 2, Loss: 2.871980905532837\n",
      "Epoch: 2, Loss: 3.384516477584839\n",
      "Epoch: 2, Loss: 2.07720685005188\n",
      "Epoch: 2, Loss: 3.9306583404541016\n",
      "Epoch: 2, Loss: 4.173415660858154\n",
      "Epoch: 2, Loss: 3.607194423675537\n",
      "Epoch: 2, Loss: 3.2053418159484863\n",
      "Epoch: 2, Loss: 3.7145514488220215\n",
      "Epoch: 2, Loss: 2.8857662677764893\n",
      "Epoch: 2, Loss: 3.6537246704101562\n",
      "Epoch: 2, Loss: 3.0901849269866943\n",
      "Epoch: 2, Loss: 3.3473429679870605\n",
      "Epoch: 2, Loss: 2.9035825729370117\n",
      "Epoch: 2, Loss: 2.953355312347412\n",
      "Epoch: 2, Loss: 1.7775126695632935\n",
      "Epoch: 2, Loss: 5.435540199279785\n",
      "Epoch: 2, Loss: 2.3102922439575195\n",
      "Epoch: 2, Loss: 4.829351425170898\n",
      "Epoch: 2, Loss: 5.34888219833374\n",
      "Epoch: 2, Loss: 3.537930488586426\n",
      "Epoch: 2, Loss: 3.826880931854248\n",
      "Epoch: 2, Loss: 4.240126132965088\n",
      "Epoch: 2, Loss: 3.738218069076538\n",
      "Epoch: 2, Loss: 2.5078186988830566\n",
      "Epoch: 2, Loss: 2.7154364585876465\n",
      "Epoch: 2, Loss: 2.352513313293457\n",
      "Epoch: 2, Loss: 4.32472038269043\n",
      "Epoch: 2, Loss: 3.1667628288269043\n",
      "Epoch: 2, Loss: 2.578122854232788\n",
      "Epoch: 2, Loss: 3.6634814739227295\n",
      "Epoch: 2, Loss: 3.40566086769104\n",
      "Epoch: 2, Loss: 4.388491153717041\n",
      "Epoch: 2, Loss: 4.926071643829346\n",
      "Epoch: 2, Loss: 4.167713642120361\n",
      "Epoch: 2, Loss: 3.9138975143432617\n",
      "Epoch: 2, Loss: 3.3934152126312256\n",
      "Epoch: 2, Loss: 4.005158424377441\n",
      "Epoch: 2, Loss: 4.37783145904541\n",
      "Epoch: 2, Loss: 3.26252818107605\n",
      "Epoch: 2, Loss: 3.913276195526123\n",
      "Epoch: 2, Loss: 2.5290472507476807\n",
      "Epoch: 2, Loss: 2.4487547874450684\n",
      "Epoch: 2, Loss: 2.72361159324646\n",
      "Epoch: 2, Loss: 2.5043785572052\n",
      "Epoch: 2, Loss: 2.097839117050171\n",
      "Epoch: 2, Loss: 3.522871255874634\n",
      "Epoch: 2, Loss: 3.349418878555298\n",
      "Epoch: 2, Loss: 3.744213104248047\n",
      "Epoch: 2, Loss: 2.8527162075042725\n",
      "Epoch: 2, Loss: 3.934222459793091\n",
      "Epoch: 2, Loss: 1.5403887033462524\n",
      "Epoch: 2, Loss: 3.262556314468384\n",
      "Epoch: 2, Loss: 4.1520867347717285\n",
      "Epoch: 2, Loss: 4.0118889808654785\n",
      "Epoch: 2, Loss: 3.1517112255096436\n",
      "Epoch: 2, Loss: 3.044434070587158\n",
      "Epoch: 2, Loss: 3.8052268028259277\n",
      "Epoch: 2, Loss: 1.7490642070770264\n",
      "Epoch: 2, Loss: 2.8781657218933105\n",
      "Epoch: 2, Loss: 2.6951534748077393\n",
      "Epoch: 2, Loss: 4.36341667175293\n",
      "Epoch: 2, Loss: 4.262199401855469\n",
      "Epoch: 2, Loss: 4.0705885887146\n",
      "Epoch: 2, Loss: 3.306215763092041\n",
      "Epoch: 2, Loss: 2.5217885971069336\n",
      "Epoch: 2, Loss: 4.205277442932129\n",
      "Epoch: 2, Loss: 3.8639793395996094\n",
      "Epoch: 2, Loss: 1.947840690612793\n",
      "Epoch: 2, Loss: 2.9205896854400635\n",
      "Epoch: 2, Loss: 5.145776271820068\n",
      "Epoch: 2, Loss: 3.8687005043029785\n",
      "Epoch: 2, Loss: 3.2664599418640137\n",
      "Epoch: 2, Loss: 3.639582872390747\n",
      "Epoch: 2, Loss: 3.786583185195923\n",
      "Epoch: 2, Loss: 1.8340967893600464\n",
      "Epoch: 2, Loss: 4.154609680175781\n",
      "Epoch: 2, Loss: 3.303577184677124\n",
      "Epoch: 2, Loss: 3.8054819107055664\n",
      "Epoch: 2, Loss: 4.318162441253662\n",
      "Epoch: 2, Loss: 2.4283742904663086\n",
      "Epoch: 2, Loss: 4.398008346557617\n",
      "Epoch: 2, Loss: 3.9986555576324463\n",
      "Epoch: 2, Loss: 2.8879051208496094\n",
      "Epoch: 2, Loss: 2.7046592235565186\n",
      "Epoch: 2, Loss: 4.19314432144165\n",
      "Epoch: 2, Loss: 4.046591758728027\n",
      "Epoch: 2, Loss: 4.881433963775635\n",
      "Epoch: 3, Loss: 3.0724403858184814\n",
      "Epoch: 3, Loss: 2.850167989730835\n",
      "Epoch: 3, Loss: 3.2844080924987793\n",
      "Epoch: 3, Loss: 3.1187994480133057\n",
      "Epoch: 3, Loss: 3.4845337867736816\n",
      "Epoch: 3, Loss: 2.293834686279297\n",
      "Epoch: 3, Loss: 3.5103836059570312\n",
      "Epoch: 3, Loss: 3.8068490028381348\n",
      "Epoch: 3, Loss: 3.013864517211914\n",
      "Epoch: 3, Loss: 2.618643283843994\n",
      "Epoch: 3, Loss: 3.669677734375\n",
      "Epoch: 3, Loss: 2.856539011001587\n",
      "Epoch: 3, Loss: 3.1221938133239746\n",
      "Epoch: 3, Loss: 2.4703166484832764\n",
      "Epoch: 3, Loss: 4.737498760223389\n",
      "Epoch: 3, Loss: 4.009433269500732\n",
      "Epoch: 3, Loss: 2.665925979614258\n",
      "Epoch: 3, Loss: 4.0056047439575195\n",
      "Epoch: 3, Loss: 3.956557273864746\n",
      "Epoch: 3, Loss: 3.2249176502227783\n",
      "Epoch: 3, Loss: 3.7195332050323486\n",
      "Epoch: 3, Loss: 3.868624687194824\n",
      "Epoch: 3, Loss: 2.435858726501465\n",
      "Epoch: 3, Loss: 3.3962879180908203\n",
      "Epoch: 3, Loss: 3.240114450454712\n",
      "Epoch: 3, Loss: 3.7463037967681885\n",
      "Epoch: 3, Loss: 3.110246181488037\n",
      "Epoch: 3, Loss: 3.8176791667938232\n",
      "Epoch: 3, Loss: 2.6568453311920166\n",
      "Epoch: 3, Loss: 3.19033145904541\n",
      "Epoch: 3, Loss: 3.745316505432129\n",
      "Epoch: 3, Loss: 5.0023298263549805\n",
      "Epoch: 3, Loss: 3.394408702850342\n",
      "Epoch: 3, Loss: 2.6425890922546387\n",
      "Epoch: 3, Loss: 2.529810905456543\n",
      "Epoch: 3, Loss: 3.3888323307037354\n",
      "Epoch: 3, Loss: 3.886413097381592\n",
      "Epoch: 3, Loss: 2.828089475631714\n",
      "Epoch: 3, Loss: 1.9487881660461426\n",
      "Epoch: 3, Loss: 1.9323631525039673\n",
      "Epoch: 3, Loss: 3.4795339107513428\n",
      "Epoch: 3, Loss: 3.505993366241455\n",
      "Epoch: 3, Loss: 4.814701557159424\n",
      "Epoch: 3, Loss: 3.5859501361846924\n",
      "Epoch: 3, Loss: 2.575779438018799\n",
      "Epoch: 3, Loss: 3.4130775928497314\n",
      "Epoch: 3, Loss: 2.960995674133301\n",
      "Epoch: 3, Loss: 2.904360771179199\n",
      "Epoch: 3, Loss: 3.4809703826904297\n",
      "Epoch: 3, Loss: 3.7462573051452637\n",
      "Epoch: 3, Loss: 4.265130043029785\n",
      "Epoch: 3, Loss: 3.2826719284057617\n",
      "Epoch: 3, Loss: 3.570181369781494\n",
      "Epoch: 3, Loss: 3.598921537399292\n",
      "Epoch: 3, Loss: 3.17681622505188\n",
      "Epoch: 3, Loss: 3.738029718399048\n",
      "Epoch: 3, Loss: 3.6375930309295654\n",
      "Epoch: 3, Loss: 4.137861251831055\n",
      "Epoch: 3, Loss: 2.922151565551758\n",
      "Epoch: 3, Loss: 3.687072277069092\n",
      "Epoch: 3, Loss: 4.3514885902404785\n",
      "Epoch: 3, Loss: 2.487396717071533\n",
      "Epoch: 3, Loss: 2.7656044960021973\n",
      "Epoch: 3, Loss: 4.118320941925049\n",
      "Epoch: 3, Loss: 4.78215217590332\n",
      "Epoch: 3, Loss: 2.5214202404022217\n",
      "Epoch: 3, Loss: 2.3131866455078125\n",
      "Epoch: 3, Loss: 2.5912818908691406\n",
      "Epoch: 3, Loss: 3.4648568630218506\n",
      "Epoch: 3, Loss: 3.2640557289123535\n",
      "Epoch: 3, Loss: 2.435157060623169\n",
      "Epoch: 3, Loss: 2.6509978771209717\n",
      "Epoch: 3, Loss: 2.1450319290161133\n",
      "Epoch: 3, Loss: 4.334426403045654\n",
      "Epoch: 3, Loss: 3.180636405944824\n",
      "Epoch: 3, Loss: 3.7466416358947754\n",
      "Epoch: 3, Loss: 3.663621664047241\n",
      "Epoch: 3, Loss: 3.820495367050171\n",
      "Epoch: 3, Loss: 2.9892187118530273\n",
      "Epoch: 3, Loss: 2.2515764236450195\n",
      "Epoch: 3, Loss: 2.0949654579162598\n",
      "Epoch: 3, Loss: 3.118609666824341\n",
      "Epoch: 3, Loss: 2.811418056488037\n",
      "Epoch: 3, Loss: 4.090936660766602\n",
      "Epoch: 3, Loss: 4.494295120239258\n",
      "Epoch: 3, Loss: 3.1090219020843506\n",
      "Epoch: 3, Loss: 3.0734739303588867\n",
      "Epoch: 3, Loss: 3.1826014518737793\n",
      "Epoch: 3, Loss: 3.811880588531494\n",
      "Epoch: 3, Loss: 3.278306007385254\n",
      "Epoch: 3, Loss: 2.8452539443969727\n",
      "Epoch: 3, Loss: 2.240619659423828\n",
      "Epoch: 3, Loss: 4.349709510803223\n",
      "Epoch: 3, Loss: 3.5098249912261963\n",
      "Epoch: 3, Loss: 2.9901108741760254\n",
      "Epoch: 3, Loss: 3.8689725399017334\n",
      "Epoch: 3, Loss: 2.202272415161133\n",
      "Epoch: 3, Loss: 4.298747539520264\n",
      "Epoch: 3, Loss: 2.6191940307617188\n",
      "Epoch: 3, Loss: 2.7859256267547607\n",
      "Epoch: 3, Loss: 2.2320027351379395\n",
      "Epoch: 3, Loss: 2.671985387802124\n",
      "Epoch: 3, Loss: 3.8779098987579346\n",
      "Epoch: 3, Loss: 3.4387922286987305\n",
      "Epoch: 3, Loss: 3.73895001411438\n",
      "Epoch: 3, Loss: 3.3486783504486084\n",
      "Epoch: 3, Loss: 3.514224052429199\n",
      "Epoch: 3, Loss: 4.065898895263672\n",
      "Epoch: 3, Loss: 3.782320976257324\n",
      "Epoch: 3, Loss: 4.27980899810791\n",
      "Epoch: 3, Loss: 4.0183868408203125\n",
      "Epoch: 3, Loss: 4.087392330169678\n",
      "Epoch: 3, Loss: 3.4373042583465576\n",
      "Epoch: 3, Loss: 3.484837770462036\n",
      "Epoch: 3, Loss: 2.1337497234344482\n",
      "Epoch: 3, Loss: 2.8786118030548096\n",
      "Epoch: 3, Loss: 3.047562837600708\n",
      "Epoch: 3, Loss: 3.5846199989318848\n",
      "Epoch: 3, Loss: 2.938715696334839\n",
      "Epoch: 3, Loss: 2.2247202396392822\n",
      "Epoch: 3, Loss: 4.165405750274658\n",
      "Epoch: 3, Loss: 3.053187370300293\n",
      "Epoch: 3, Loss: 3.9556150436401367\n",
      "Epoch: 3, Loss: 4.028334617614746\n",
      "Epoch: 3, Loss: 4.053423881530762\n",
      "Epoch: 3, Loss: 3.432058334350586\n",
      "Epoch: 3, Loss: 3.665903091430664\n",
      "Epoch: 3, Loss: 1.2410314083099365\n",
      "Epoch: 3, Loss: 1.8789443969726562\n",
      "Epoch: 3, Loss: 1.4240636825561523\n",
      "Epoch: 3, Loss: 4.062872886657715\n",
      "Epoch: 3, Loss: 3.783010482788086\n",
      "Epoch: 3, Loss: 2.2556650638580322\n",
      "Epoch: 3, Loss: 2.709059715270996\n",
      "Epoch: 3, Loss: 2.6171345710754395\n",
      "Epoch: 3, Loss: 3.774629592895508\n",
      "Epoch: 3, Loss: 3.4401016235351562\n",
      "Epoch: 3, Loss: 3.335414409637451\n",
      "Epoch: 3, Loss: 3.094666004180908\n",
      "Epoch: 3, Loss: 2.737875461578369\n",
      "Epoch: 3, Loss: 2.9013524055480957\n",
      "Epoch: 3, Loss: 3.137993097305298\n",
      "Epoch: 3, Loss: 3.3104710578918457\n",
      "Epoch: 3, Loss: 0.7981773018836975\n",
      "Epoch: 3, Loss: 3.9307327270507812\n",
      "Epoch: 3, Loss: 2.7507517337799072\n",
      "Epoch: 3, Loss: 3.3098199367523193\n",
      "Epoch: 3, Loss: 2.691046714782715\n",
      "Epoch: 3, Loss: 2.676365852355957\n",
      "Epoch: 3, Loss: 2.5206634998321533\n",
      "Epoch: 3, Loss: 4.595654487609863\n",
      "Epoch: 3, Loss: 3.5737144947052\n",
      "Epoch: 3, Loss: 4.3683648109436035\n",
      "Epoch: 3, Loss: 2.8041841983795166\n",
      "Epoch: 3, Loss: 4.298415184020996\n",
      "Epoch: 3, Loss: 4.013892650604248\n",
      "Epoch: 3, Loss: 3.150184392929077\n",
      "Epoch: 3, Loss: 3.320117712020874\n",
      "Epoch: 3, Loss: 3.3413374423980713\n",
      "Epoch: 3, Loss: 3.782888889312744\n",
      "Epoch: 3, Loss: 2.961188316345215\n",
      "Epoch: 3, Loss: 3.586320400238037\n",
      "Epoch: 3, Loss: 3.5376832485198975\n",
      "Epoch: 3, Loss: 5.2341694831848145\n",
      "Epoch: 3, Loss: 2.962904214859009\n",
      "Epoch: 3, Loss: 3.540236473083496\n",
      "Epoch: 3, Loss: 2.5377397537231445\n",
      "Epoch: 3, Loss: 2.35595703125\n",
      "Epoch: 3, Loss: 2.164463996887207\n",
      "Epoch: 3, Loss: 3.3436484336853027\n",
      "Epoch: 3, Loss: 3.72105073928833\n",
      "Epoch: 3, Loss: 3.2439992427825928\n",
      "Epoch: 3, Loss: 2.6384894847869873\n",
      "Epoch: 3, Loss: 2.753187417984009\n",
      "Epoch: 3, Loss: 4.582371711730957\n",
      "Epoch: 3, Loss: 4.867512226104736\n",
      "Epoch: 3, Loss: 3.644707441329956\n",
      "Epoch: 3, Loss: 3.3033530712127686\n",
      "Epoch: 3, Loss: 4.630825996398926\n",
      "Epoch: 3, Loss: 3.7096097469329834\n",
      "Epoch: 3, Loss: 3.4006664752960205\n",
      "Epoch: 3, Loss: 4.420572757720947\n",
      "Epoch: 3, Loss: 3.783107042312622\n",
      "Epoch: 3, Loss: 4.452116966247559\n",
      "Epoch: 3, Loss: 5.320318222045898\n",
      "Epoch: 3, Loss: 4.485126972198486\n",
      "Epoch: 3, Loss: 3.281615972518921\n",
      "Epoch: 3, Loss: 3.4699931144714355\n",
      "Epoch: 3, Loss: 3.3088889122009277\n",
      "Epoch: 3, Loss: 4.203068256378174\n",
      "Epoch: 3, Loss: 3.1234993934631348\n",
      "Epoch: 3, Loss: 4.5462117195129395\n",
      "Epoch: 3, Loss: 4.659475326538086\n",
      "Epoch: 3, Loss: 3.712892770767212\n",
      "Epoch: 3, Loss: 4.880241394042969\n",
      "Epoch: 3, Loss: 3.785841941833496\n",
      "Epoch: 3, Loss: 2.474540948867798\n",
      "Epoch: 3, Loss: 4.589057922363281\n",
      "Epoch: 3, Loss: 3.345716953277588\n",
      "Epoch: 3, Loss: 4.466940879821777\n",
      "Epoch: 3, Loss: 3.0679776668548584\n",
      "Epoch: 3, Loss: 3.334399938583374\n",
      "Epoch: 3, Loss: 3.417335033416748\n",
      "Epoch: 3, Loss: 3.5493099689483643\n",
      "Epoch: 3, Loss: 3.3196938037872314\n",
      "Epoch: 3, Loss: 2.713630199432373\n",
      "Epoch: 3, Loss: 2.121760845184326\n",
      "Epoch: 3, Loss: 3.860379219055176\n",
      "Epoch: 3, Loss: 3.0616953372955322\n",
      "Epoch: 3, Loss: 4.269375324249268\n",
      "Epoch: 3, Loss: 2.2553675174713135\n",
      "Epoch: 3, Loss: 1.482012391090393\n",
      "Epoch: 3, Loss: 3.3561530113220215\n",
      "Epoch: 3, Loss: 3.9137661457061768\n",
      "Epoch: 3, Loss: 4.456155776977539\n",
      "Epoch: 3, Loss: 4.3170976638793945\n",
      "Epoch: 3, Loss: 3.685343027114868\n",
      "Epoch: 3, Loss: 3.0194153785705566\n",
      "Epoch: 3, Loss: 4.083768844604492\n",
      "Epoch: 3, Loss: 2.819110870361328\n",
      "Epoch: 3, Loss: 2.406496286392212\n",
      "Epoch: 3, Loss: 4.798966884613037\n",
      "Epoch: 3, Loss: 1.75351881980896\n",
      "Epoch: 3, Loss: 3.146027088165283\n",
      "Epoch: 3, Loss: 2.6175642013549805\n",
      "Epoch: 3, Loss: 4.307548522949219\n",
      "Epoch: 3, Loss: 3.1279032230377197\n",
      "Epoch: 3, Loss: 5.008554935455322\n",
      "Epoch: 3, Loss: 2.2399063110351562\n",
      "Epoch: 3, Loss: 3.3577892780303955\n",
      "Epoch: 3, Loss: 3.8275179862976074\n",
      "Epoch: 3, Loss: 2.3245761394500732\n",
      "Epoch: 3, Loss: 2.572314739227295\n",
      "Epoch: 3, Loss: 3.5353472232818604\n",
      "Epoch: 3, Loss: 3.7466483116149902\n",
      "Epoch: 3, Loss: 2.211416482925415\n",
      "Epoch: 3, Loss: 4.009115219116211\n",
      "Epoch: 3, Loss: 3.9308722019195557\n",
      "Epoch: 3, Loss: 2.228591203689575\n",
      "Epoch: 3, Loss: 2.5416736602783203\n",
      "Epoch: 3, Loss: 2.8689959049224854\n",
      "Epoch: 3, Loss: 3.329411029815674\n",
      "Epoch: 3, Loss: 3.3012402057647705\n",
      "Epoch: 3, Loss: 3.19775128364563\n",
      "Epoch: 3, Loss: 4.887522220611572\n",
      "Epoch: 3, Loss: 2.289564847946167\n",
      "Epoch: 3, Loss: 1.6075031757354736\n",
      "Epoch: 3, Loss: 2.4574339389801025\n",
      "Epoch: 3, Loss: 3.7719388008117676\n",
      "Epoch: 3, Loss: 4.433924674987793\n",
      "Epoch: 3, Loss: 3.963865041732788\n",
      "Epoch: 3, Loss: 3.5848100185394287\n",
      "Epoch: 3, Loss: 3.3770928382873535\n",
      "Epoch: 3, Loss: 5.015622138977051\n",
      "Epoch: 3, Loss: 2.8456830978393555\n",
      "Epoch: 3, Loss: 3.7291438579559326\n",
      "Epoch: 3, Loss: 1.9693186283111572\n",
      "Epoch: 3, Loss: 2.7707109451293945\n",
      "Epoch: 3, Loss: 4.104974269866943\n",
      "Epoch: 3, Loss: 1.9675374031066895\n",
      "Epoch: 3, Loss: 3.702436923980713\n",
      "Epoch: 4, Loss: 4.614598274230957\n",
      "Epoch: 4, Loss: 3.0899817943573\n",
      "Epoch: 4, Loss: 3.811444044113159\n",
      "Epoch: 4, Loss: 3.2683422565460205\n",
      "Epoch: 4, Loss: 4.659731864929199\n",
      "Epoch: 4, Loss: 2.8953914642333984\n",
      "Epoch: 4, Loss: 3.0816493034362793\n",
      "Epoch: 4, Loss: 3.126429557800293\n",
      "Epoch: 4, Loss: 4.057854652404785\n",
      "Epoch: 4, Loss: 2.8555173873901367\n",
      "Epoch: 4, Loss: 2.650073289871216\n",
      "Epoch: 4, Loss: 2.520608901977539\n",
      "Epoch: 4, Loss: 2.6176035404205322\n",
      "Epoch: 4, Loss: 2.850245952606201\n",
      "Epoch: 4, Loss: 2.72635555267334\n",
      "Epoch: 4, Loss: 2.3754048347473145\n",
      "Epoch: 4, Loss: 3.5341734886169434\n",
      "Epoch: 4, Loss: 4.1914472579956055\n",
      "Epoch: 4, Loss: 4.058830738067627\n",
      "Epoch: 4, Loss: 3.013570785522461\n",
      "Epoch: 4, Loss: 4.1164703369140625\n",
      "Epoch: 4, Loss: 4.106100559234619\n",
      "Epoch: 4, Loss: 3.4724836349487305\n",
      "Epoch: 4, Loss: 2.270963668823242\n",
      "Epoch: 4, Loss: 3.9721901416778564\n",
      "Epoch: 4, Loss: 4.014400005340576\n",
      "Epoch: 4, Loss: 2.750844717025757\n",
      "Epoch: 4, Loss: 3.472478151321411\n",
      "Epoch: 4, Loss: 2.233623504638672\n",
      "Epoch: 4, Loss: 3.0906434059143066\n",
      "Epoch: 4, Loss: 3.138195037841797\n",
      "Epoch: 4, Loss: 2.946463108062744\n",
      "Epoch: 4, Loss: 2.612677574157715\n",
      "Epoch: 4, Loss: 4.59035062789917\n",
      "Epoch: 4, Loss: 3.8734726905822754\n",
      "Epoch: 4, Loss: 3.1046552658081055\n",
      "Epoch: 4, Loss: 2.326883316040039\n",
      "Epoch: 4, Loss: 4.080554962158203\n",
      "Epoch: 4, Loss: 2.443075180053711\n",
      "Epoch: 4, Loss: 2.1394588947296143\n",
      "Epoch: 4, Loss: 2.207566499710083\n",
      "Epoch: 4, Loss: 5.299924373626709\n",
      "Epoch: 4, Loss: 3.1709823608398438\n",
      "Epoch: 4, Loss: 2.2691056728363037\n",
      "Epoch: 4, Loss: 2.620793342590332\n",
      "Epoch: 4, Loss: 3.4219369888305664\n",
      "Epoch: 4, Loss: 3.4992265701293945\n",
      "Epoch: 4, Loss: 4.484800338745117\n",
      "Epoch: 4, Loss: 2.728588581085205\n",
      "Epoch: 4, Loss: 3.9393563270568848\n",
      "Epoch: 4, Loss: 3.806097984313965\n",
      "Epoch: 4, Loss: 2.411557912826538\n",
      "Epoch: 4, Loss: 2.1772420406341553\n",
      "Epoch: 4, Loss: 3.9561033248901367\n",
      "Epoch: 4, Loss: 3.361936092376709\n",
      "Epoch: 4, Loss: 4.08092737197876\n",
      "Epoch: 4, Loss: 4.087824821472168\n",
      "Epoch: 4, Loss: 4.618748664855957\n",
      "Epoch: 4, Loss: 1.931829810142517\n",
      "Epoch: 4, Loss: 3.228271484375\n",
      "Epoch: 4, Loss: 3.832435369491577\n",
      "Epoch: 4, Loss: 3.018237352371216\n",
      "Epoch: 4, Loss: 2.7320473194122314\n",
      "Epoch: 4, Loss: 2.5737946033477783\n",
      "Epoch: 4, Loss: 4.549868106842041\n",
      "Epoch: 4, Loss: 2.5610358715057373\n",
      "Epoch: 4, Loss: 1.4762649536132812\n",
      "Epoch: 4, Loss: 3.6216280460357666\n",
      "Epoch: 4, Loss: 3.354782819747925\n",
      "Epoch: 4, Loss: 3.1841814517974854\n",
      "Epoch: 4, Loss: 3.950984477996826\n",
      "Epoch: 4, Loss: 2.5759029388427734\n",
      "Epoch: 4, Loss: 3.4973995685577393\n",
      "Epoch: 4, Loss: 2.8465576171875\n",
      "Epoch: 4, Loss: 3.181304931640625\n",
      "Epoch: 4, Loss: 4.276468276977539\n",
      "Epoch: 4, Loss: 2.7274606227874756\n",
      "Epoch: 4, Loss: 3.665649652481079\n",
      "Epoch: 4, Loss: 2.5499603748321533\n",
      "Epoch: 4, Loss: 2.8376107215881348\n",
      "Epoch: 4, Loss: 3.497410774230957\n",
      "Epoch: 4, Loss: 2.323077917098999\n",
      "Epoch: 4, Loss: 3.0460660457611084\n",
      "Epoch: 4, Loss: 3.071833610534668\n",
      "Epoch: 4, Loss: 3.140376329421997\n",
      "Epoch: 4, Loss: 3.0696964263916016\n",
      "Epoch: 4, Loss: 3.604388475418091\n",
      "Epoch: 4, Loss: 1.9291272163391113\n",
      "Epoch: 4, Loss: 3.293137788772583\n",
      "Epoch: 4, Loss: 4.113363265991211\n",
      "Epoch: 4, Loss: 4.369870185852051\n",
      "Epoch: 4, Loss: 1.7785673141479492\n",
      "Epoch: 4, Loss: 4.279085159301758\n",
      "Epoch: 4, Loss: 3.983710765838623\n",
      "Epoch: 4, Loss: 2.875527858734131\n",
      "Epoch: 4, Loss: 3.0318374633789062\n",
      "Epoch: 4, Loss: 3.696272373199463\n",
      "Epoch: 4, Loss: 2.5745577812194824\n",
      "Epoch: 4, Loss: 3.046553611755371\n",
      "Epoch: 4, Loss: 3.0607850551605225\n",
      "Epoch: 4, Loss: 3.3031513690948486\n",
      "Epoch: 4, Loss: 3.6734261512756348\n",
      "Epoch: 4, Loss: 3.0634477138519287\n",
      "Epoch: 4, Loss: 3.116560935974121\n",
      "Epoch: 4, Loss: 3.5938098430633545\n",
      "Epoch: 4, Loss: 3.087679386138916\n",
      "Epoch: 4, Loss: 1.4618184566497803\n",
      "Epoch: 4, Loss: 2.3197576999664307\n",
      "Epoch: 4, Loss: 1.3562240600585938\n",
      "Epoch: 4, Loss: 2.102236747741699\n",
      "Epoch: 4, Loss: 2.2932143211364746\n",
      "Epoch: 4, Loss: 4.566589832305908\n",
      "Epoch: 4, Loss: 2.7367641925811768\n",
      "Epoch: 4, Loss: 3.3887155055999756\n",
      "Epoch: 4, Loss: 3.5285236835479736\n",
      "Epoch: 4, Loss: 3.310194969177246\n",
      "Epoch: 4, Loss: 2.804274559020996\n",
      "Epoch: 4, Loss: 3.736132860183716\n",
      "Epoch: 4, Loss: 3.1776907444000244\n",
      "Epoch: 4, Loss: 3.343768358230591\n",
      "Epoch: 4, Loss: 3.6580026149749756\n",
      "Epoch: 4, Loss: 4.25560188293457\n",
      "Epoch: 4, Loss: 3.5587716102600098\n",
      "Epoch: 4, Loss: 3.210275173187256\n",
      "Epoch: 4, Loss: 3.072810649871826\n",
      "Epoch: 4, Loss: 4.7524542808532715\n",
      "Epoch: 4, Loss: 3.194220542907715\n",
      "Epoch: 4, Loss: 3.6684720516204834\n",
      "Epoch: 4, Loss: 2.3330321311950684\n",
      "Epoch: 4, Loss: 3.5408172607421875\n",
      "Epoch: 4, Loss: 4.058255195617676\n",
      "Epoch: 4, Loss: 2.59576416015625\n",
      "Epoch: 4, Loss: 2.734394073486328\n",
      "Epoch: 4, Loss: 2.5254082679748535\n",
      "Epoch: 4, Loss: 4.5843400955200195\n",
      "Epoch: 4, Loss: 3.4284610748291016\n",
      "Epoch: 4, Loss: 2.869659185409546\n",
      "Epoch: 4, Loss: 2.3077785968780518\n",
      "Epoch: 4, Loss: 2.496706008911133\n",
      "Epoch: 4, Loss: 3.4754090309143066\n",
      "Epoch: 4, Loss: 1.9878151416778564\n",
      "Epoch: 4, Loss: 2.8934245109558105\n",
      "Epoch: 4, Loss: 3.190471887588501\n",
      "Epoch: 4, Loss: 2.923107862472534\n",
      "Epoch: 4, Loss: 2.5815441608428955\n",
      "Epoch: 4, Loss: 3.3699779510498047\n",
      "Epoch: 4, Loss: 2.872725486755371\n",
      "Epoch: 4, Loss: 4.420695781707764\n",
      "Epoch: 4, Loss: 3.7139546871185303\n",
      "Epoch: 4, Loss: 3.6986563205718994\n",
      "Epoch: 4, Loss: 3.2314038276672363\n",
      "Epoch: 4, Loss: 3.6005747318267822\n",
      "Epoch: 4, Loss: 4.951337814331055\n",
      "Epoch: 4, Loss: 3.588080406188965\n",
      "Epoch: 4, Loss: 4.28101921081543\n",
      "Epoch: 4, Loss: 3.842528820037842\n",
      "Epoch: 4, Loss: 4.451419353485107\n",
      "Epoch: 4, Loss: 3.191770553588867\n",
      "Epoch: 4, Loss: 1.346946358680725\n",
      "Epoch: 4, Loss: 6.127162933349609\n",
      "Epoch: 4, Loss: 4.718743324279785\n",
      "Epoch: 4, Loss: 3.0479507446289062\n",
      "Epoch: 4, Loss: 3.7744264602661133\n",
      "Epoch: 4, Loss: 2.818753957748413\n",
      "Epoch: 4, Loss: 3.413139820098877\n",
      "Epoch: 4, Loss: 3.431109666824341\n",
      "Epoch: 4, Loss: 4.1584858894348145\n",
      "Epoch: 4, Loss: 3.4109315872192383\n",
      "Epoch: 4, Loss: 4.241721153259277\n",
      "Epoch: 4, Loss: 3.3990776538848877\n",
      "Epoch: 4, Loss: 5.391056060791016\n",
      "Epoch: 4, Loss: 2.0744788646698\n",
      "Epoch: 4, Loss: 2.5548505783081055\n",
      "Epoch: 4, Loss: 3.944410562515259\n",
      "Epoch: 4, Loss: 2.207061767578125\n",
      "Epoch: 4, Loss: 3.034327983856201\n",
      "Epoch: 4, Loss: 3.292898654937744\n",
      "Epoch: 4, Loss: 2.463768482208252\n",
      "Epoch: 4, Loss: 3.0314040184020996\n",
      "Epoch: 4, Loss: 4.180527687072754\n",
      "Epoch: 4, Loss: 2.5193567276000977\n",
      "Epoch: 4, Loss: 2.4586615562438965\n",
      "Epoch: 4, Loss: 1.9705973863601685\n",
      "Epoch: 4, Loss: 3.316560745239258\n",
      "Epoch: 4, Loss: 3.4048891067504883\n",
      "Epoch: 4, Loss: 3.3791534900665283\n",
      "Epoch: 4, Loss: 4.160438537597656\n",
      "Epoch: 4, Loss: 2.916243076324463\n",
      "Epoch: 4, Loss: 2.498962879180908\n",
      "Epoch: 4, Loss: 3.1067793369293213\n",
      "Epoch: 4, Loss: 3.8212172985076904\n",
      "Epoch: 4, Loss: 4.391472339630127\n",
      "Epoch: 4, Loss: 3.584456443786621\n",
      "Epoch: 4, Loss: 3.250859498977661\n",
      "Epoch: 4, Loss: 2.7580742835998535\n",
      "Epoch: 4, Loss: 3.1249685287475586\n",
      "Epoch: 4, Loss: 4.410616874694824\n",
      "Epoch: 4, Loss: 4.1851396560668945\n",
      "Epoch: 4, Loss: 3.274493455886841\n",
      "Epoch: 4, Loss: 2.8738222122192383\n",
      "Epoch: 4, Loss: 2.3986222743988037\n",
      "Epoch: 4, Loss: 3.8845009803771973\n",
      "Epoch: 4, Loss: 2.758145809173584\n",
      "Epoch: 4, Loss: 2.5282394886016846\n",
      "Epoch: 4, Loss: 4.181581020355225\n",
      "Epoch: 4, Loss: 2.804067611694336\n",
      "Epoch: 4, Loss: 2.269685745239258\n",
      "Epoch: 4, Loss: 2.323259115219116\n",
      "Epoch: 4, Loss: 3.382741928100586\n",
      "Epoch: 4, Loss: 3.9130043983459473\n",
      "Epoch: 4, Loss: 3.96167254447937\n",
      "Epoch: 4, Loss: 3.530625104904175\n",
      "Epoch: 4, Loss: 2.365767240524292\n",
      "Epoch: 4, Loss: 4.587246894836426\n",
      "Epoch: 4, Loss: 4.187978267669678\n",
      "Epoch: 4, Loss: 1.989856481552124\n",
      "Epoch: 4, Loss: 5.169364929199219\n",
      "Epoch: 4, Loss: 4.51310396194458\n",
      "Epoch: 4, Loss: 2.782869577407837\n",
      "Epoch: 4, Loss: 2.557615280151367\n",
      "Epoch: 4, Loss: 3.795475482940674\n",
      "Epoch: 4, Loss: 3.2119617462158203\n",
      "Epoch: 4, Loss: 2.9975991249084473\n",
      "Epoch: 4, Loss: 3.1141936779022217\n",
      "Epoch: 4, Loss: 2.9960031509399414\n",
      "Epoch: 4, Loss: 4.660148620605469\n",
      "Epoch: 4, Loss: 4.071863174438477\n",
      "Epoch: 4, Loss: 3.3342366218566895\n",
      "Epoch: 4, Loss: 2.486769676208496\n",
      "Epoch: 4, Loss: 3.27479887008667\n",
      "Epoch: 4, Loss: 1.8427636623382568\n",
      "Epoch: 4, Loss: 3.5362062454223633\n",
      "Epoch: 4, Loss: 2.910492420196533\n",
      "Epoch: 4, Loss: 3.0764734745025635\n",
      "Epoch: 4, Loss: 2.5843281745910645\n",
      "Epoch: 4, Loss: 3.803870916366577\n",
      "Epoch: 4, Loss: 2.823342800140381\n",
      "Epoch: 4, Loss: 5.134034156799316\n",
      "Epoch: 4, Loss: 2.9443304538726807\n",
      "Epoch: 4, Loss: 3.3382880687713623\n",
      "Epoch: 4, Loss: 4.58527946472168\n",
      "Epoch: 4, Loss: 2.889983892440796\n",
      "Epoch: 4, Loss: 3.4307658672332764\n",
      "Epoch: 4, Loss: 3.6331191062927246\n",
      "Epoch: 4, Loss: 2.1736412048339844\n",
      "Epoch: 4, Loss: 4.740909099578857\n",
      "Epoch: 4, Loss: 3.2162513732910156\n",
      "Epoch: 4, Loss: 3.0915377140045166\n",
      "Epoch: 4, Loss: 3.1732728481292725\n",
      "Epoch: 4, Loss: 2.4038846492767334\n",
      "Epoch: 4, Loss: 3.1245343685150146\n",
      "Epoch: 4, Loss: 3.088088035583496\n",
      "Epoch: 4, Loss: 2.701399326324463\n",
      "Epoch: 4, Loss: 3.098877191543579\n",
      "Epoch: 4, Loss: 4.0035481452941895\n",
      "Epoch: 4, Loss: 3.2991442680358887\n",
      "Epoch: 4, Loss: 3.6879849433898926\n",
      "Epoch: 4, Loss: 2.871567964553833\n",
      "Epoch: 4, Loss: 4.006148815155029\n",
      "Epoch: 4, Loss: 2.6494295597076416\n",
      "Epoch: 4, Loss: 3.585681438446045\n",
      "Training complete.\n",
      "Validation Accuracy: 0.3429\n",
      "Validation Precision: 0.2100\n",
      "Validation Recall: 0.3429\n",
      "Validation F1 Score: 0.2471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\11\\anaconda3\\envs\\wedrive\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\11\\anaconda3\\envs\\wedrive\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#분기로 train/test 데이터 분할\n",
    "# Encode the target variable End_grid\n",
    "label_encoder = LabelEncoder()\n",
    "all_labels = list(train_data['end_grid']) + list(test_data['end_grid'])\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "train_data['end_grid_encoded'] = label_encoder.transform(train_data['end_grid'])\n",
    "test_data['end_grid_encoded'] = label_encoder.transform(test_data['end_grid'])\n",
    "\n",
    "# Extract the features and target\n",
    "train_X = train_data['start_grid'].tolist()\n",
    "train_y = train_data['end_grid_encoded'].tolist()\n",
    "test_X = test_data['start_grid'].tolist()\n",
    "test_y = test_data['end_grid_encoded'].tolist()\n",
    "'''\n",
    "\n",
    "#랜덤으로 분할\n",
    "# Encode the target variable F1\n",
    "label_encoder = LabelEncoder()\n",
    "dataframe['end_grid_encoded'] = label_encoder.fit_transform(dataframe['end_grid'])\n",
    "\n",
    "# Extract the features and target\n",
    "X = dataframe['start_grid'].tolist()\n",
    "y = dataframe['end_grid_encoded'].tolist()\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Custom Dataset class for our data\n",
    "class GridPathDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Parameters\n",
    "MAX_LEN = 8\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Create dataset\n",
    "dataset = GridPathDataset(X, y, tokenizer, MAX_LEN)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Model training setup\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "EPOCHS = 5\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "val_targets = []\n",
    "val_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "        val_targets.extend(labels.cpu().numpy())\n",
    "        val_predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(val_targets, val_predictions)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(val_targets, val_predictions, average='weighted')\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "print(f'Validation Precision: {precision:.4f}')\n",
    "print(f'Validation Recall: {recall:.4f}')\n",
    "print(f'Validation F1 Score: {f1:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wedrive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
